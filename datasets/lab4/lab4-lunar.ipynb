{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARI 510 Lab 4, Lunar Lander"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ryan Smith, 12/3/24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project for cartpole and lunar lander - Reinforcement Learning Project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mainly using PyTorch for the DQN and related functions.  NumPy, OpenAI's Gymnasium for cartpole and lunar lander.  Imageio with ffmpeg for the video capturing.  Data Structure wise, deque from the python standard library collections module.  Also the standard library random module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the DQN Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines the architecture of the Deep Q-Network (DQN) used to approximate the Q-value function. The DQN class inherits from nn.Module, which is the base class for all neural networks in PyTorch. The init method initializes the network's layers, creating a fully connected (nn.Linear) network with two hidden layers, each having 128 neurons and ReLU activation functions (nn.ReLU). The input dimension (input_dim) corresponds to the size of the state space, while the output dimension (output_dim) matches the number of possible actions. The forward method defines the forward pass of the network, taking a state as input (x) and passing it through the sequence of layers (self.fc) to produce the estimated Q-values for each action. This output is then used to select actions during the agent's interaction with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DQN neural network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section sets up the Lunar Lander environment for the reinforcement learning task. env = gym.make(\"LunarLander-v3\", render_mode='rgb_array') creates an instance of the Lunar Lander environment from the OpenAI Gym library, specifically version 3. The render_mode='rgb_array' argument configures the environment to provide RGB image arrays as visual output, which can be used for generating videos or displaying the agent's behavior. state_dim = env.observation_space.shape[0] retrieves the dimensionality of the state space, which represents the number of variables used to describe the environment's state (e.g., lander position, velocity, angle).  action_dim = env.action_space.n gets the number of possible actions the agent can take in the environment (e.g., fire left engine, fire main engine, fire right engine, do nothing). This information is crucial for defining the architecture of the DQN model and for action selection during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment\n",
    "env = gym.make(\"LunarLander-v3\", render_mode='rgb_array')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet initializes the core components of the DQN algorithm. model is the main Deep Q-Network, which is responsible for estimating the Q-values of different actions in given states. target_model is a copy of the main model, used to provide stable target Q-values during training.  target_model.load_state_dict(model.state_dict()) synchronizes the weights of the target model with the main model initially.  optimizer = optim.Adam(model.parameters(), lr=1e-3) creates an Adam optimizer to update the weights of the main model during training, using a learning rate of 0.001. replay_buffer = deque(maxlen=100000) creates a replay buffer, which is a data structure that stores the agent's experiences (state, action, reward, next state, done) for later sampling during training. This helps break temporal correlations in the data and improves learning stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model and target model\n",
    "model = DQN(state_dim, action_dim)\n",
    "target_model = DQN(state_dim, action_dim)\n",
    "target_model.load_state_dict(model.state_dict())\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "replay_buffer = deque(maxlen=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section defines the key parameters that govern the learning process of the DQN agent. gamma determines how much the agent values future rewards compared to immediate ones. batch_size sets the number of experiences sampled from the replay buffer for each training step. epsilon controls the exploration-exploitation balance, with higher values leading to more random actions. epsilon_min sets a lower bound for exploration, ensuring the agent continues to explore even after extensive training. epsilon_decay controls how quickly the exploration rate decreases over time. update_target_every specifies how often the target network's weights are updated with the main network's weights.  Finally, rolling_window sets the number of recent episodes to consider when calculating rolling average metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "gamma = 0.99\n",
    "batch_size = 32\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.1\n",
    "epsilon_decay = 0.995\n",
    "update_target_every = 100\n",
    "rolling_window = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function defines the core training logic for the DQN agent. It first checks if the replay buffer has enough experiences (batch_size) to sample from. If not, it returns without training. Otherwise, it randomly samples a batch of experiences from the replay buffer. This batch includes the agent's states, actions, received rewards, next states, and whether the episode ended after each action. These elements are then converted into PyTorch tensors for efficient computation. The function calculates the predicted Q-values for the taken actions using the main DQN model. Then, it uses the target network to estimate the optimal Q-values for the next states.  These are used to calculate target Q-values, incorporating the reward and discounted future rewards. The difference between the predicted and target Q-values forms the loss, which is used to update the main DQN model's weights through backpropagation. This process optimizes the model to make better decisions over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train():\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return\n",
    "    batch = random.sample(replay_buffer, batch_size)\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "    states = torch.FloatTensor(states)\n",
    "    actions = torch.LongTensor(actions)\n",
    "    rewards = torch.FloatTensor(rewards)\n",
    "    next_states = torch.FloatTensor(next_states)\n",
    "    dones = torch.FloatTensor(dones)\n",
    "\n",
    "    q_values = model(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
    "    with torch.no_grad():\n",
    "        next_q_values = target_model(next_states).max(1)[0]\n",
    "        targets = rewards + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "    loss = nn.MSELoss()(q_values, targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Function - Select Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select action function\n",
    "def select_action(state):\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        q_values = model(state_tensor)\n",
    "        return torch.argmax(q_values).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section initializes lists and variables to store and track the agent's performance throughout the training process. steps_per_episode records the number of steps taken in each episode. success_count and total_success_count keep track of successful landings within a rolling window and overall, respectively. rewards_list stores the total reward obtained in each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "steps_per_episode = []\n",
    "success_count = 0\n",
    "total_success_count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains the main training loop that iterates through a specified number of episodes (num_episodes). Inside the loop, the environment is reset, and for each step, the agent selects an action, observes the next state and reward, stores the experience in the replay buffer, and trains the DQN model. It also updates the epsilon value for exploration and periodically updates the target network.  The loop tracks total reward and steps per episode, and updates success rate counters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wt/wht0n7qd6cx0p5f47rhf_qnh0000gn/T/ipykernel_12472/1875768712.py:8: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:281.)\n",
      "  states = torch.FloatTensor(states)\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Reward: -132.1918690988905, Avg Reward (last 250): -132.1918690988905, Overall Avg Reward: -132.1918690988905, Steps: 98, Avg Steps: 98.0, Success Rate (last 250): 0.00%, Overall Success Rate: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 250, Reward: -103.23468146314636, Avg Reward (last 250): -148.72564335879218, Overall Avg Reward: -148.659771748195, Steps: 238, Avg Steps: 204.988, Success Rate (last 250): 0.00%, Overall Success Rate: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 500, Reward: -287.9941073432341, Avg Reward (last 250): -110.99472198326845, Overall Avg Reward: -129.86483673575657, Steps: 146, Avg Steps: 508.356, Success Rate (last 250): 6.80%, Overall Success Rate: 3.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 750, Reward: -260.9884660730068, Avg Reward (last 250): -152.87165689620497, Overall Avg Reward: -137.52356515135193, Steps: 1000, Avg Steps: 767.736, Success Rate (last 250): 14.40%, Overall Success Rate: 7.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000, Reward: -200.67236100671474, Avg Reward (last 250): -205.85316258856452, Overall Avg Reward: -154.58889917662978, Steps: 316, Avg Steps: 841.644, Success Rate (last 250): 1.20%, Overall Success Rate: 5.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1250, Reward: -134.41790324440944, Avg Reward (last 250): -174.82446405815247, Overall Avg Reward: -158.63277705063513, Steps: 104, Avg Steps: 554.18, Success Rate (last 250): 0.80%, Overall Success Rate: 4.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1500, Reward: -84.65438876708137, Avg Reward (last 250): -154.3361645287428, Overall Avg Reward: -157.91715204698883, Steps: 1000, Avg Steps: 543.144, Success Rate (last 250): 0.40%, Overall Success Rate: 3.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1750, Reward: -164.10632596628432, Avg Reward (last 250): -138.5261120272875, Overall Avg Reward: -155.14858551076648, Steps: 269, Avg Steps: 447.16, Success Rate (last 250): 2.80%, Overall Success Rate: 3.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2000, Reward: -138.91246729754113, Avg Reward (last 250): -135.21944993532827, Overall Avg Reward: -152.6586885123359, Steps: 167, Avg Steps: 449.904, Success Rate (last 250): 4.40%, Overall Success Rate: 3.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2250, Reward: 31.63713290652568, Avg Reward (last 250): -126.11838684429011, Overall Avg Reward: -149.711076154712, Steps: 1000, Avg Steps: 544.236, Success Rate (last 250): 7.60%, Overall Success Rate: 4.26%\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "rewards_list = []\n",
    "global epsilon\n",
    "num_episodes = 10000\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    landed_successfully = False\n",
    "\n",
    "    while True:\n",
    "        action = select_action(state)\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        terminal = done or truncated\n",
    "\n",
    "        # Check if the landing was successful\n",
    "        if done and reward == 100:\n",
    "            landed_successfully = True\n",
    "\n",
    "        # Intermediate rewards for partial achievements\n",
    "        if not done:\n",
    "            reward += -0.1 * np.linalg.norm(state[:2])  # Penalize distance from the landing pad\n",
    "\n",
    "        replay_buffer.append((state, action, reward, next_state, terminal))\n",
    "        train()\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        if terminal:\n",
    "            break\n",
    "\n",
    "    rewards_list.append(total_reward)\n",
    "    steps_per_episode.append(steps)\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "    # Update success rate\n",
    "    if landed_successfully:\n",
    "        success_count += 1\n",
    "        total_success_count += 1\n",
    "\n",
    "    # Update target model\n",
    "    if episode % update_target_every == 0:\n",
    "        target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "    # Generate video and metrics every 250 episodes\n",
    "    if episode % 250 == 0:\n",
    "        frames = []\n",
    "        state, info = env.reset()\n",
    "        for _ in range(1000):\n",
    "            frames.append(env.render())\n",
    "            action = select_action(state)\n",
    "            next_state, _, done, truncated, _ = env.step(action)\n",
    "            state = next_state\n",
    "            if done or truncated:\n",
    "                break\n",
    "        imageio.mimsave(f\"lunarlander_episode_{episode}.mp4\", frames, fps=30)\n",
    "\n",
    "        # Metrics and Logging\n",
    "        avg_reward = np.mean(rewards_list[-rolling_window:])\n",
    "        avg_steps = np.mean(steps_per_episode[-rolling_window:])\n",
    "        success_rate = (success_count / rolling_window) * 100  # Rolling success rate as percentage\n",
    "        overall_avg_reward = np.mean(rewards_list)\n",
    "        overall_success_rate = (total_success_count / (episode + 1)) * 100  # Overall success rate as percentage\n",
    "\n",
    "        print(f\"Episode {episode}, Reward: {total_reward}, Avg Reward (last {rolling_window}): {avg_reward}, \"\n",
    "              f\"Overall Avg Reward: {overall_avg_reward}, Steps: {steps}, Avg Steps: {avg_steps}, \"\n",
    "              f\"Success Rate (last {rolling_window}): {success_rate:.2f}%, Overall Success Rate: {overall_success_rate:.2f}%\")\n",
    "\n",
    "        # Reset rolling success count\n",
    "        success_count = 0\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
