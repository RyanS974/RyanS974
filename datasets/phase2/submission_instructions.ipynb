{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4MiNUKVK1Zq"
   },
   "source": [
    "# Submission Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f16ItCuZK4L2"
   },
   "source": [
    "Here are instructions on how to submit your validation dataset based model results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYKt0EXJslyU"
   },
   "source": [
    "In this GitHub directory is the training set, named 'training_set_labeled.csv', and the validation set you get your submission results from, named 'validation_set_unlabeled.csv', along with 'test_set_unlabeled.csv' which is the final dataset the preds.txt is based on.  Load the training set into this python notebook in a following section that is an example, or your own python code, then train your model.  With that, run your model on the validation set tuning hyperparameters and analyzing metrics, then run it on the test set and print the resulting labels to a file named 'preds.txt'.  Then simply submit that in a zipped file named 'preds.txt.zip' on the CodaLabs page.  The example combine.py file is a basic example to show the process, which only uses the training set and test set to then print the preds.txt.  Ideally you use the validation set also for hyperparameter tuning and metrics analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2c7vEKYYixY"
   },
   "source": [
    "There are some various other files in the GitHub directory related to the project also."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HbdfgYEwtZWl"
   },
   "source": [
    "# Example Code for Loading the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0YVHvGAAExET",
    "outputId": "cddd8b85-fa54-408f-fd8d-f8ee6a60e0b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (600, 6) (600, 2)\n",
      "Validation set: (400, 6)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the training and validation datasets from GitHub\n",
    "train_dataset_url = 'https://raw.githubusercontent.com/RyanS974/RyanS974/main/datasets/phase2/training_set_labeled.csv'\n",
    "test_dataset_url = 'https://raw.githubusercontent.com/RyanS974/RyanS974/main/datasets/phase2/test_set_unlabeled.csv'\n",
    "\n",
    "# column names\n",
    "column_names = [\"id\", \"skills\", \"exp\", \"grades\", \"projects\", \"extra\", \"offer\", \"hire\", \"pay\"]\n",
    "column_names2 = [\"id\", \"skills\", \"exp\", \"grades\", \"projects\", \"extra\", \"offer\"]\n",
    "\n",
    "# Load the datasets\n",
    "train_data = pd.read_csv(train_dataset_url, names=column_names).drop(columns=[\"id\"])\n",
    "test_data = pd.read_csv(test_dataset_url, names=column_names2).drop(columns=[\"id\"])\n",
    "\n",
    "# Separate features and labels for the training dataset\n",
    "X_train = train_data.drop(columns=[\"hire\", \"pay\"])  # Features for training\n",
    "y_train = train_data[[\"hire\", \"pay\"]]               # Labels for training\n",
    "\n",
    "# Validation features without dropping any additional columns\n",
    "X_test = test_data  # Features for validation\n",
    "\n",
    "# Print shapes to verify the split\n",
    "print(\"Training set:\", X_train.shape, y_train.shape)\n",
    "print(\"Test set:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L76iI-ZrthZa"
   },
   "source": [
    "# Example Code for Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "bVAbMk5Ytkan",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "e9185560-24fb-4607-cf2a-734563758c55"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['Interview' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['Yes' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['Interview' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['Yes' '125']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['Interview' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['Yes' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['Yes' '125']\n",
      " ['Interview' '150']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['Interview' '150']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['Yes' '125']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['Interview' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['Interview' '150']\n",
      " ['No' 'NoPay']\n",
      " ['Interview' '125']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['Interview' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['Yes' '125']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['Interview' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['Yes' '125']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['Yes' '125']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['Yes' '150']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['Yes' '125']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['Yes' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['Interview' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['Interview' '150']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']\n",
      " ['No' 'NoPay']]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for encoding\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Initialize CountVectorizer for skills column\n",
    "vectorizer = CountVectorizer(tokenizer=lambda x: x.split(\";\"))\n",
    "\n",
    "# Set up the preprocessor with CountVectorizer for the skills column\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"skills\", vectorizer, \"skills\")\n",
    "    ],\n",
    "    remainder=\"passthrough\"  # Keeps the other (numerical) columns as they are\n",
    ")\n",
    "\n",
    "# Create a pipeline to combine preprocessing and model training\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", MultiOutputClassifier(RandomForestClassifier(n_estimators=300, max_depth=None, max_features=1.0, random_state=42)))\n",
    "])\n",
    "\n",
    "# Train the model on the training set\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set for an example, skipping validation\n",
    "y_test_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Print prediction labels\n",
    "print(y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9R4Nm_Utk9s"
   },
   "source": [
    "# Submitting Your Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgrCniJStob1"
   },
   "source": [
    "The results from the previous section can be copied into a text file and submitted. Name the file preds.txt also."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V97P9QN-1oIB"
   },
   "source": [
    "# Ideas for Your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_8yFvlo1sE8"
   },
   "source": [
    "You can use any classifier you want, and also you can tune the hyperparameters or add new ones.  My example uses random forest, with just three hyperparameters.  At the most basic level you could tune those hyperparameters and try to get the best possible results to submit.\n",
    "\n",
    "Also, my GitHub directory has python code files of these steps which you can access and modify or run on your own outside of this Google Colab Python Notebook.\n",
    "\n",
    "The scoring will be done on CodaLabs through a scoring script file after you submit the results.  The results are needed to be submitted in the format of \"No,Interview\" for example.  This is without any quotes in the text file, the quotes are used above to specify the line entry.  This would be one for each line.  The validation set has 400 datapoints so it would be 400.  Some of the other combinations are \"Yes,125\", etc...\n",
    "\n",
    "Name this file preds.txt and submit it on the CodaLabs page for this competition, and it will score it on the leaderboard."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
