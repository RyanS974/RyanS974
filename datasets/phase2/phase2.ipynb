{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project Phase 2"
      ],
      "metadata": {
        "id": "8VnUWZeUnjcG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phase 2 of our project."
      ],
      "metadata": {
        "id": "uGKREJQInm6F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2. Compute Agreement Between Annotators"
      ],
      "metadata": {
        "id": "DJtsVpW8noIt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use Cohen's Kappa for the compute agreement which is good for cases of two annotators.  The other options of Fleiss' Kappa, Krippendorff's Alpha, and Percentage Agreement do not seem as good of choices in our case.\n",
        "\n",
        "As a side note, the label 'None' for the column 'pay' had to be changed to 'NoPay' because Pandas was interpreting that as a NaN.  Instead of doing a work around in Pandas, which was possible, I decided to just change the label name to make it easier for others to work with the dataset."
      ],
      "metadata": {
        "id": "i6ejWr1hBCBj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cohen's Kappa"
      ],
      "metadata": {
        "id": "pKlkpvRJEXXr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For pairwise agreement between two annotators.  We will compute each of the two labels separately, first for 'hire', then for 'pay'."
      ],
      "metadata": {
        "id": "NClHGAYfEZCi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute for Hire"
      ],
      "metadata": {
        "id": "oPtKuCClJB4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# Load the CSV files from the GitHub repository\n",
        "url_annotator1 = 'https://raw.githubusercontent.com/RyanS974/RyanS974/main/datasets/phase2/zoiya.csv'\n",
        "url_annotator2 = 'https://raw.githubusercontent.com/RyanS974/RyanS974/main/datasets/phase2/meriem.csv'\n",
        "\n",
        "# Read the data\n",
        "annotator1 = pd.read_csv(url_annotator1)\n",
        "annotator2 = pd.read_csv(url_annotator2)\n",
        "\n",
        "# Ensure both dataframes are sorted and indexed in the same way\n",
        "annotator1 = annotator1.sort_values(by=\"id\").reset_index(drop=True)\n",
        "annotator2 = annotator2.sort_values(by=\"id\").reset_index(drop=True)\n",
        "\n",
        "# Extract labels\n",
        "labels_annotator1 = annotator1['hire']\n",
        "labels_annotator2 = annotator2['hire']\n",
        "\n",
        "# Compute Cohen's Kappa\n",
        "kappa_score = cohen_kappa_score(labels_annotator1, labels_annotator2)\n",
        "\n",
        "# Print the result and interpretation\n",
        "print(f\"Cohen's Kappa Score: {kappa_score:.2f}\")\n",
        "\n",
        "# Interpret the Kappa score\n",
        "if kappa_score >= 0.75:\n",
        "    print(\"Interpretation: Strong agreement\")\n",
        "elif 0.6 <= kappa_score < 0.75:\n",
        "    print(\"Interpretation: Moderate agreement\")\n",
        "elif 0.4 <= kappa_score < 0.6:\n",
        "    print(\"Interpretation: Fair agreement\")\n",
        "else:\n",
        "    print(\"Interpretation: Poor agreement\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sh5mo28JIaCg",
        "outputId": "326b8679-ffc4-4b78-9371-9869b19a4aa1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 0.68\n",
            "Interpretation: Moderate agreement\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This score is of a moderate agreement between the annotators in the 'hire' column (label)."
      ],
      "metadata": {
        "id": "U1CfF8irJEhu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute for Pay"
      ],
      "metadata": {
        "id": "oxBMbJMQJGfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# Load the CSV files from the GitHub repository\n",
        "url_annotator1 = 'https://raw.githubusercontent.com/RyanS974/RyanS974/main/datasets/phase2/zoiya.csv'\n",
        "url_annotator2 = 'https://raw.githubusercontent.com/RyanS974/RyanS974/main/datasets/phase2/meriem.csv'\n",
        "\n",
        "# Read the data\n",
        "annotator1 = pd.read_csv(url_annotator1)\n",
        "annotator2 = pd.read_csv(url_annotator2)\n",
        "\n",
        "# Extract labels\n",
        "labels_annotator1 = annotator1['pay']\n",
        "labels_annotator2 = annotator2['pay']\n",
        "\n",
        "# Compute Cohen's Kappa\n",
        "kappa_score = cohen_kappa_score(labels_annotator1, labels_annotator2)\n",
        "\n",
        "# Print the result and interpretation\n",
        "print(f\"Cohen's Kappa Score: {kappa_score:.2f}\")\n",
        "\n",
        "# Interpret the Kappa score\n",
        "if kappa_score >= 0.75:\n",
        "    print(\"Interpretation: Strong agreement\")\n",
        "elif 0.6 <= kappa_score < 0.75:\n",
        "    print(\"Interpretation: Moderate agreement\")\n",
        "elif 0.4 <= kappa_score < 0.6:\n",
        "    print(\"Interpretation: Fair agreement\")\n",
        "else:\n",
        "    print(\"Interpretation: Poor agreement\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_g43_LPJQTH",
        "outputId": "bca9a125-af82-46da-e471-070892f97620"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 0.38\n",
            "Interpretation: Poor agreement\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the 'pay' column (label) we have a poor agreement between the annotators."
      ],
      "metadata": {
        "id": "Bb3BWvTAJH6w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpretation"
      ],
      "metadata": {
        "id": "Qrjo_CMdEcp-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though there is what is rated as 'poor agreement' with our 'pay' label, this is not that bad.  There is a large amount of interpretation ability with that.  The algorithm for Cohen's Kappa is taking into account the large number of NoPay matches and altering the score, I believe.  The score would normally be higher here.  This is the first time I have worked with Cohen's Kappa but I believe that is the explanation."
      ],
      "metadata": {
        "id": "RPKSZafWEedC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 3. Determine Ground Truth Labels"
      ],
      "metadata": {
        "id": "E2hOyZqNBB1G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our ground truth labels are the final labels for the dataset.  There are several options available, but the best choice for this case was the 'majority voting' method, which is essentially just a third annotator, myself."
      ],
      "metadata": {
        "id": "iVxY3kvhBL3U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Majority Voting"
      ],
      "metadata": {
        "id": "Y3WNXFrpD64E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I annotated the data as a third annotator which helped resolve any ties.  The full_dataset.csv file is of the final 'ground truth' labels for the dataset."
      ],
      "metadata": {
        "id": "_fwV_ESVD89c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 4. Analyze Data"
      ],
      "metadata": {
        "id": "xLmOelMiBMKL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "k_2gcOHuBQO5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 5. Setup Codabench Page for Task"
      ],
      "metadata": {
        "id": "kqNex9v3CooL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YZ7vn95KC2sx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task Definition"
      ],
      "metadata": {
        "id": "Wzlb5Lz0C32M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CodaBench has a basic Task Definition concept which is given below."
      ],
      "metadata": {
        "id": "LmpGYToSC50n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actual Task Definition"
      ],
      "metadata": {
        "id": "31Ozb4K035Im"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task Name: Job Candidate Hiring and Salary Prediction\n",
        "\n",
        "Objective: Predict whether a candidate will be hired based on their skills, experience, and other features, and estimate their salary if they are hired.\n",
        "\n",
        "Dataset: The dataset includes information on candidate skills, years of experience, grades, number of completed projects, and involvement in extracurriculars. Labels include \"hire\" (Yes, No, Interview) and \"pay\" (a value or \"NoPay\" if not hired).\n",
        "\n",
        "Expected Output: A binary prediction for \"hire\" and, if positive, a prediction for \"pay.\"\n",
        "\n",
        "Evaluation Metrics: Models will be evaluated based on F1-score.\n",
        "\n",
        "Example Input/Output:\n",
        "\n",
        "Input: Skills: SQL, Machine Learning, Java; Experience: 3 years; Grades: 92; Projects: 3; Extra: 2\n",
        "Output: Hire: No, Pay: NoPay"
      ],
      "metadata": {
        "id": "ZyuBX72n373h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split Data"
      ],
      "metadata": {
        "id": "P2ED2MxJC6RI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will work with a standard 60 20 20 dataset split, for training, validation, and testing."
      ],
      "metadata": {
        "id": "w-D9tGuRC76_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "dataset1 = 'https://raw.githubusercontent.com/RyanS974/RyanS974/main/datasets/phase2/full_dataset.csv'\n",
        "data = pd.read_csv(dataset1)\n",
        "\n",
        "# Separate the features and labels\n",
        "X = data.drop(columns=[\"hire\", \"pay\"])  # Drop target columns to get features\n",
        "y = data[[\"hire\", \"pay\"]]  # Target columns (multi-label)\n",
        "\n",
        "# First, split the data into training and temp (which will be split into validation and test sets)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "\n",
        "# Now split the temp set equally into validation and test sets (20% each of the original data)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Print shapes to verify the split\n",
        "print(\"Training set:\", X_train.shape, y_train.shape)\n",
        "print(\"Validation set:\", X_val.shape, y_val.shape)\n",
        "print(\"Test set:\", X_test.shape, y_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vV8l6PR4rcHR",
        "outputId": "335e8b80-7a1e-4ddd-e166-67b9a31f8c3b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set: (600, 7) (600, 2)\n",
            "Validation set: (200, 7) (200, 2)\n",
            "Test set: (200, 7) (200, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Metric"
      ],
      "metadata": {
        "id": "SvrK1fX_C8KA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use f1-score for both 'hire' and 'pay'.  This for both classification labels (\"hire\" and \"pay\") is a great choice, especially since it balances precision and recall."
      ],
      "metadata": {
        "id": "2wwAALOhDAHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline Model"
      ],
      "metadata": {
        "id": "GsXACanaDAb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use a Random Forest classifier for our baseline model, with some basic hyperparameter tuning to make it better than the dummy baseline we will create later.  We will also need to use a multi-label wrapper also since we have two labels.\n",
        "\n",
        "We will also combine the training, validation, and testing in one code block below, with basic print outs of the f1-scores for validation and testing."
      ],
      "metadata": {
        "id": "JQziF56hDCCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for encoding\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Initialize CountVectorizer for skills column\n",
        "vectorizer = CountVectorizer(tokenizer=lambda x: x.split(\";\"))\n",
        "\n",
        "# Set up the preprocessor with CountVectorizer for the skills column\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"skills\", vectorizer, \"skills\")\n",
        "    ],\n",
        "    remainder=\"passthrough\"  # Keeps the other (numerical) columns as they are\n",
        ")\n",
        "\n",
        "# Create a pipeline to combine preprocessing and model training\n",
        "pipeline = Pipeline([\n",
        "    (\"preprocess\", preprocessor),\n",
        "    (\"model\", MultiOutputClassifier(RandomForestClassifier(n_estimators=300, max_depth=None, max_features=1.0, random_state=42)))\n",
        "])\n",
        "\n",
        "# Train the model on the training set\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_val_pred = pipeline.predict(X_val)\n",
        "\n",
        "# Calculate F1-score for each label on the validation set\n",
        "f1_hire_val = f1_score(y_val['hire'], y_val_pred[:, 0], average=\"weighted\")\n",
        "f1_pay_val = f1_score(y_val['pay'], y_val_pred[:, 1], average=\"weighted\")\n",
        "\n",
        "print(\"Validation F1-score for 'hire' label:\", f1_hire_val)\n",
        "print(\"Validation F1-score for 'pay' label:\", f1_pay_val)\n",
        "\n",
        "# Make predictions on the test set to evaluate generalization\n",
        "y_test_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Calculate F1-score for each label on the test set\n",
        "f1_hire_test = f1_score(y_test['hire'], y_test_pred[:, 0], average=\"weighted\")\n",
        "f1_pay_test = f1_score(y_test['pay'], y_test_pred[:, 1], average=\"weighted\")\n",
        "\n",
        "print(\"\\nTest F1-score for 'hire' label:\", f1_hire_test)\n",
        "print(\"Test F1-score for 'pay' label:\", f1_pay_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIhTOl_DrgGC",
        "outputId": "37611c25-0391-4312-c066-a1d86160630b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1-score for 'hire' label: 0.99\n",
            "Validation F1-score for 'pay' label: 0.9659289013858205\n",
            "\n",
            "Test F1-score for 'hire' label: 0.9894642857142857\n",
            "Test F1-score for 'pay' label: 0.9780191815856777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have somewhat of a class imbalance issue in our labels that is being shown in these f1-scores.  They are very high, but normal in our scenario.  We have many 'NoPay' and 'No' labels which are the main reason for this.  There is no way around this without changing the nature of the project, so I believe the class imbalance is essentially unavoidable and these scores are acceptable.\n",
        "\n",
        "There is a small discrepancy of f1-score results in the the two sets of validation and test.  This is normal and acceptable also.  They are both composed of 200 entries, with the entries not being identical.  We went from about .988 to .99 from test to validation with the 'hire', and a similar difference in the other."
      ],
      "metadata": {
        "id": "BXhj7Y2pHApH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameters"
      ],
      "metadata": {
        "id": "PSl19Pdu9JhH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the main hyperparameters for the Random Forest classifier we will use:\n",
        "\n",
        "**n_estimators**: The number of trees directly affects the model's ensemble power. It's usually the first parameter to tune since increasing it improves performance up to a point but with a diminishing return.\n",
        "\n",
        "**max_depth**: Controlling the tree depth is crucial for managing complexity and preventing overfitting, especially on small or noisy datasets. Shallower trees reduce overfitting, while deeper trees can capture more data nuances.\n",
        "\n",
        "**max_features**: This parameter impacts diversity across trees. Lower values help create trees that specialize in different parts of the feature space, which generally leads to better generalization.\n",
        "\n",
        "***In our above baseline model we just trained and evaluated, those settings should be on the higher end for f1-scoring.  In our dummy baseline below, we will use more random and default settings and will give lower scores.***"
      ],
      "metadata": {
        "id": "i0ydoaEb9JTl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our validation metric is f1-score for both classification labels.  This is what the codabench competition is based on."
      ],
      "metadata": {
        "id": "xMkVPHTp9X7j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 6. Dummy Baseline (Random Baseline)"
      ],
      "metadata": {
        "id": "-uyUrVhKDEpO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is our dummy baseline which is the Random based one.  We will use default or random hyperparameters values also."
      ],
      "metadata": {
        "id": "eaoFyj5FDHrV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training, Validation, and Testing of Dummy Baseline"
      ],
      "metadata": {
        "id": "LsJsNZt3DH9u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YfhNtNSjDJt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize CountVectorizer for skills column\n",
        "vectorizer = CountVectorizer(tokenizer=lambda x: x.split(\";\"))\n",
        "\n",
        "# Set up the preprocessor with CountVectorizer for the skills column\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"skills\", vectorizer, \"skills\")\n",
        "    ],\n",
        "    remainder=\"passthrough\"  # Keeps the other (numerical) columns as they are\n",
        ")\n",
        "\n",
        "# Create a pipeline to combine preprocessing and model training\n",
        "pipeline = Pipeline([\n",
        "    (\"preprocess\", preprocessor),\n",
        "    (\"model\", MultiOutputClassifier(RandomForestClassifier(n_estimators=75, max_depth=15, max_features=\"sqrt\", random_state=42)))\n",
        "])\n",
        "\n",
        "# Train the model on the training set\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_val_pred = pipeline.predict(X_val)\n",
        "\n",
        "# Calculate F1-score for each label on the validation set\n",
        "f1_hire_val = f1_score(y_val['hire'], y_val_pred[:, 0], average=\"weighted\")\n",
        "f1_pay_val = f1_score(y_val['pay'], y_val_pred[:, 1], average=\"weighted\")\n",
        "\n",
        "print(\"Validation F1-score for 'hire' label:\", f1_hire_val)\n",
        "print(\"Validation F1-score for 'pay' label:\", f1_pay_val)\n",
        "\n",
        "# Make predictions on the test set to evaluate generalization\n",
        "y_test_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Calculate F1-score for each label on the test set\n",
        "f1_hire_test = f1_score(y_test['hire'], y_test_pred[:, 0], average=\"weighted\")\n",
        "f1_pay_test = f1_score(y_test['pay'], y_test_pred[:, 1], average=\"weighted\")\n",
        "\n",
        "print(\"\\nTest F1-score for 'hire' label:\", f1_hire_test)\n",
        "print(\"Test F1-score for 'pay' label:\", f1_pay_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUWuKMm8-Z34",
        "outputId": "e21ee00c-d498-4963-e5d8-bf45f1be2b89"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1-score for 'hire' label: 0.9947899159663866\n",
            "Validation F1-score for 'pay' label: 0.9404081632653061\n",
            "\n",
            "Test F1-score for 'hire' label: 0.9775263157894737\n",
            "Test F1-score for 'pay' label: 0.9626582278481013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These hyperparameter settings that are different, mainly of a random nature and default nature, are performing worse than our baseline.  Not by much, but it is lower.  In our CodaBench competition, it is allowing for comparisons still, which is the main point.  We do, as mentioned earlier, have somewhat of a class imbalance issue which is causing our high scores, but due to the nature of the project, it is not really avoidable, making the high scores acceptable."
      ],
      "metadata": {
        "id": "pvObzNHm-jj9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trained Model vs Dummy Model"
      ],
      "metadata": {
        "id": "mQrv2XCEDKC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Earlier, we trained and validated the baseline.  We just trained and validated our dummy baseline which was intended to be simpler.  Below is a more detailed comparison."
      ],
      "metadata": {
        "id": "nuqMPNCXDMTu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The comparison between the Dummy Baseline and Actual Baseline highlights the effects of model complexity on F1-scores for both labels, \"hire\" and \"pay.\" The Dummy Baseline, configured with a moderate number of trees (n_estimators=75), a limited depth (max_depth=15), and a subset of features (max_features=\"sqrt\"), performs exceptionally well for the \"hire\" label, achieving a slightly higher F1-score on the validation set than the Actual Baseline. This suggests that the \"hire\" labelâ€™s patterns are well captured with a simpler model, which balances performance with generalization. However, the \"pay\" label demonstrates a clear benefit from added model complexity, as evidenced by higher validation and test F1-scores with the Actual Baseline.\n",
        "\n",
        "The Actual Baseline, which leverages a higher number of trees (n_estimators=300), unlimited depth (max_depth=None), and full feature usage (max_features=1.0), delivers the highest F1-scores on the test set for both labels, showing it can generalize well across unseen data. This indicates that the \"pay\" label likely contains more nuanced patterns that a simpler model might miss, making the added complexity of the Actual Baseline worthwhile. In scenarios where computational resources allow, the Actual Baseline is the better choice for maximizing predictive power, particularly on the \"pay\" label. However, if computational efficiency is prioritized, the Dummy Baseline is a solid alternative, delivering high F1-scores with fewer resources."
      ],
      "metadata": {
        "id": "cRAByfRfDNAP"
      }
    }
  ]
}