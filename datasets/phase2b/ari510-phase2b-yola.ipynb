{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b967f6a-3c82-4f14-bdf8-ccb5032a0211",
   "metadata": {},
   "source": [
    "# ARI 510 Phase 2b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0562f4a8-c746-4838-8109-7e2a788966be",
   "metadata": {},
   "source": [
    "Project Phase 2b Competition Participation\n",
    "Ryan Smith\n",
    "Project Competing in: Yola's team's titled \"Bacterial Morphology Classification\" which is an image classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933c469d",
   "metadata": {},
   "source": [
    "At the very bottom of this python notebook there are headings of Full Code for Model 1, and Full Code for Model 2.  The beginning sections here break the code up for explanatory purposes.  You can run the full code from those bottom headings.  There is also multiple full code blocks you can run with different settings listed.\n",
    "\n",
    "To run these, download the datasets in the GitHub directory that this python notebook is in.  The dataset and file loading is from a local copy, not directly from GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3856954",
   "metadata": {},
   "source": [
    "# Data Loading, Hyperparameters, Preprocessing, and Debug Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecaa945",
   "metadata": {},
   "source": [
    "Below we will do a number things, including setting hyperparameters, load the datasets, preprocess the images, and print some debug information.  Part of the hyperparameters are preprocessing, technically, such as scaling and resizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cf31c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 360 images belonging to 3 classes.\n",
      "Found 120 images belonging to 3 classes.\n",
      "Total training images: 360\n",
      "Total validation images: 120\n",
      "Total test images: 120\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Preprocessing parameters\n",
    "target_size = (224, 224)  # Resize images to 224x224 pixels\n",
    "rescale_factor = 1.0 / 255  # Normalize pixel values to [0, 1]\n",
    "\n",
    "# Define directories\n",
    "train_dir = 'yola/split_dataset/train'\n",
    "validation_dir = 'yola/split_dataset/validation'\n",
    "test_dir = 'yola/split_dataset/test'\n",
    "test_filenames_path = 'yola/test_filenames.txt'\n",
    "\n",
    "# Function to preprocess images\n",
    "def preprocess_images(directory, target_size, rescale_factor):\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(('jpg', 'jpeg', 'png')):\n",
    "                img_path = os.path.join(root, file)\n",
    "                img = load_img(img_path)\n",
    "                img = img.resize(target_size)\n",
    "                img_array = img_to_array(img) * rescale_factor  # Normalize pixel values to [0, 1]\n",
    "                img = array_to_img(img_array)\n",
    "                img.save(img_path)\n",
    "\n",
    "# Preprocess images in training, validation, and test directories\n",
    "preprocess_images(train_dir, target_size, rescale_factor)\n",
    "preprocess_images(validation_dir, target_size, rescale_factor)\n",
    "preprocess_images(test_dir, target_size, rescale_factor)\n",
    "\n",
    "# Data generators for training, validation, and test datasets\n",
    "datagen = ImageDataGenerator()  # No additional preprocessing needed\n",
    "\n",
    "# Load datasets\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=target_size,  # Images are already resized to 224x224 pixels\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=target_size,  # Images are already resized to 224x224 pixels\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Debug information\n",
    "print(f\"Total training images: {train_generator.samples}\")\n",
    "print(f\"Total validation images: {validation_generator.samples}\")\n",
    "\n",
    "# Count total test images\n",
    "with open(test_filenames_path, 'r') as file:\n",
    "    test_filenames = file.read().splitlines()\n",
    "\n",
    "print(f\"Total test images: {len(test_filenames)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2e4eb2",
   "metadata": {},
   "source": [
    "# Check Image Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5fe3da",
   "metadata": {},
   "source": [
    "Below is some debug and information code to check the image sizes.  They vary drastically from about 150x150 to 1000sx1000x.  We resize them to 224x224 for almost all tests, since it performs better than 150x150, with those two values being standards.  \n",
    "\n",
    "IMPORTANT NOTE: \n",
    "\n",
    "Also, this will print 224x224 here, since the images are already resized.  It needs to be inserted before the image resizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9663299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique training image sizes: {(224, 224)}\n",
      "Unique validation image sizes: {(224, 224)}\n",
      "Unique test image sizes: {(224, 224)}\n"
     ]
    }
   ],
   "source": [
    "# from PIL import Image\n",
    "\n",
    "def check_image_sizes(directory):\n",
    "    sizes = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(('jpg', 'jpeg', 'png')):\n",
    "                img_path = os.path.join(root, file)\n",
    "                with Image.open(img_path) as img:\n",
    "                    sizes.append(img.size)\n",
    "    return sizes\n",
    "\n",
    "# Check image sizes in training, validation, and test directories\n",
    "train_sizes = check_image_sizes(train_dir)\n",
    "validation_sizes = check_image_sizes(validation_dir)\n",
    "test_sizes = check_image_sizes(test_dir)\n",
    "\n",
    "# Print unique sizes\n",
    "print(f\"Unique training image sizes: {set(train_sizes)}\")\n",
    "print(f\"Unique validation image sizes: {set(validation_sizes)}\")\n",
    "print(f\"Unique test image sizes: {set(test_sizes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0180dc66",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276c6ebf",
   "metadata": {},
   "source": [
    "Below is Data Augmentation code that does not test well.  The image shifts and rotations and flips lower the accuracy considerably in my tests.  It is displayed below for reference purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10299c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "\n",
    "# Data augmentation for training and basic scaling for validation/testing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,  # Normalize pixel values to [0, 1]\n",
    "    rotation_range=20,  # Random rotations\n",
    "    width_shift_range=0.2,  # Horizontal shifts\n",
    "    height_shift_range=0.2,  # Vertical shifts\n",
    "    horizontal_flip=True  # Random horizontal flips\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e7d11b",
   "metadata": {},
   "source": [
    "# Model 1 Building (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae334658",
   "metadata": {},
   "source": [
    "Below we will build our first model which is a CNN model using the Keras implementation Sequential model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d4f956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1 building\n",
    "\n",
    "# Model definition\n",
    "model = Sequential([\n",
    "    Input(shape=(224, 224, 3)),\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(3, activation='softmax')  # Assuming 3 classes: bacilli, cocci, spirilla\n",
    "])\n",
    "\n",
    "# Compile the model with the specified learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a14c6c",
   "metadata": {},
   "source": [
    "# Model 1 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dd0a71",
   "metadata": {},
   "source": [
    "Below is the training code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83464740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 1 training\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d0a22b",
   "metadata": {},
   "source": [
    "# Model 1 Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab084d4",
   "metadata": {},
   "source": [
    "Below is validation code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348f3c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1 validation\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "validation_steps = validation_generator.samples // batch_size\n",
    "val_loss, val_accuracy = model.evaluate(validation_generator, steps=validation_steps)\n",
    "print(f\"Validation loss: {val_loss}\")\n",
    "print(f\"Validation accuracy: {val_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8ae136",
   "metadata": {},
   "source": [
    "# Model 1 testing and printing to file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b42f75",
   "metadata": {},
   "source": [
    "Below is the testing code and printing to file code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bb113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "test_images = []\n",
    "for filename in test_filenames:\n",
    "    img_path = os.path.join(test_dir, filename)\n",
    "    img = load_img(img_path, target_size=target_size)\n",
    "    img_array = img_to_array(img) * rescale_factor  # Normalize pixel values to [0, 1]\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    test_images.append(img_array)\n",
    "\n",
    "test_images = np.vstack(test_images)\n",
    "\n",
    "print(f\"Total test images being processed: {len(test_images)}\")\n",
    "\n",
    "# Predict on test data\n",
    "predictions = model.predict(test_images)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Save predictions to a file\n",
    "output_file = 'preds.txt'\n",
    "with open(output_file, 'w') as f:\n",
    "    for label in predicted_classes:\n",
    "        f.write(f\"{label}\\n\")\n",
    "\n",
    "print(f\"Predictions saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995da120-81e5-46bb-a0f1-11065aa6af6c",
   "metadata": {},
   "source": [
    "# Model 2 (Transfer Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fb6450",
   "metadata": {},
   "source": [
    "Much of the code for the transfer learning model is the same as the previous model's.  I won't go over each individual component for this one.  The full code is below under the heading of 'Full Code for Model 2', along with various tunings of it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ec4c7f",
   "metadata": {},
   "source": [
    "# Full Code for Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8531bc9",
   "metadata": {},
   "source": [
    "Here is various full code views for model 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452017c9",
   "metadata": {},
   "source": [
    "## with data augmentation and image sizes of 224x224:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04104127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 360 images belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 120 images belonging to 3 classes.\n",
      "Total training images: 360\n",
      "Total validation images: 120\n",
      "Total test images: 120\n",
      "Epoch 1/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 485ms/step - accuracy: 0.3195 - loss: 4.4469 - val_accuracy: 0.3229 - val_loss: 1.1246\n",
      "Epoch 2/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.2812 - loss: 1.1812 - val_accuracy: 0.3750 - val_loss: 1.1010\n",
      "Epoch 3/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 475ms/step - accuracy: 0.3704 - loss: 1.0954 - val_accuracy: 0.4062 - val_loss: 1.1001\n",
      "Epoch 4/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.2812 - loss: 1.1098 - val_accuracy: 0.3333 - val_loss: 1.0759\n",
      "Epoch 5/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 478ms/step - accuracy: 0.3494 - loss: 1.0950 - val_accuracy: 0.3333 - val_loss: 1.0883\n",
      "Epoch 6/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.3750 - loss: 1.0917 - val_accuracy: 0.3333 - val_loss: 1.1098\n",
      "Epoch 7/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 475ms/step - accuracy: 0.3999 - loss: 1.0977 - val_accuracy: 0.3125 - val_loss: 1.1317\n",
      "Epoch 8/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.3438 - loss: 1.0982 - val_accuracy: 0.4167 - val_loss: 0.9751\n",
      "Epoch 9/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 453ms/step - accuracy: 0.3180 - loss: 1.0972 - val_accuracy: 0.4688 - val_loss: 1.0684\n",
      "Epoch 10/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4062 - loss: 1.0773 - val_accuracy: 0.3333 - val_loss: 1.0945\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.4336 - loss: 1.0851\n",
      "Validation loss: 1.0825647115707397\n",
      "Validation accuracy: 0.4375\n",
      "Total test images being processed: 120\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
      "Predictions saved to preds.txt\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Define directories\n",
    "train_dir = 'yola/split_dataset/train'\n",
    "validation_dir = 'yola/split_dataset/validation'\n",
    "test_dir = 'yola/split_dataset/test'\n",
    "test_filenames_path = 'yola/test_filenames.txt'\n",
    "\n",
    "\n",
    "# Data augmentation for training and basic scaling for validation/testing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,  # Normalize pixel values to [0, 1]\n",
    "    rotation_range=20,  # Random rotations\n",
    "    width_shift_range=0.2,  # Horizontal shifts\n",
    "    height_shift_range=0.2,  # Vertical shifts\n",
    "    horizontal_flip=True  # Random horizontal flips\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "# Load datasets\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),  # Resize images to 224x224 pixels\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(224, 224),  # Resize images to 224x224 pixels\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Debug information\n",
    "print(f\"Total training images: {train_generator.samples}\")\n",
    "print(f\"Total validation images: {validation_generator.samples}\")\n",
    "\n",
    "# Count total test images\n",
    "with open(test_filenames_path, 'r') as file:\n",
    "    test_filenames = file.read().splitlines()\n",
    "\n",
    "print(f\"Total test images: {len(test_filenames)}\")\n",
    "\n",
    "# Model definition\n",
    "model = Sequential([\n",
    "    Input(shape=(224, 224, 3)),\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(3, activation='softmax')  # Assuming 3 classes: bacilli, cocci, spirilla\n",
    "])\n",
    "\n",
    "# Compile the model with the specified learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size\n",
    ")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "validation_steps = validation_generator.samples // batch_size\n",
    "val_loss, val_accuracy = model.evaluate(validation_generator, steps=validation_steps)\n",
    "print(f\"Validation loss: {val_loss}\")\n",
    "print(f\"Validation accuracy: {val_accuracy}\")\n",
    "\n",
    "# Predict on test data\n",
    "test_images = []\n",
    "for filename in test_filenames:\n",
    "    img_path = os.path.join(test_dir, filename)\n",
    "    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    test_images.append(img_array)\n",
    "\n",
    "test_images = np.vstack(test_images)\n",
    "\n",
    "print(f\"Total test images being processed: {len(test_images)}\")\n",
    "\n",
    "# Predict on test data\n",
    "predictions = model.predict(test_images)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Save predictions to a file\n",
    "output_file = 'preds.txt'\n",
    "with open(output_file, 'w') as f:\n",
    "    for label in predicted_classes:\n",
    "        f.write(f\"{label}\\n\")\n",
    "\n",
    "print(f\"Predictions saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd648a1",
   "metadata": {},
   "source": [
    "## 150x150 image sizes below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b72b98a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique training image sizes: {(1119, 721), (999, 1000), (700, 499), (1045, 643), (1280, 720), (315, 160), (640, 640), (367, 372), (498, 318), (700, 462), (612, 459), (1024, 931), (300, 300), (841, 538), (419, 400), (201, 251), (513, 691), (1500, 988), (1000, 951), (1300, 1061), (500, 373), (186, 271), (1280, 1003), (1433, 1600), (1083, 800), (426, 378), (1024, 778), (1616, 1216), (1024, 723), (242, 160), (563, 484), (259, 194), (188, 207), (473, 355), (270, 203), (542, 418), (1300, 1289), (1500, 1225), (280, 180), (2079, 2040), (1485, 1600), (366, 213), (500, 238), (1000, 887), (197, 255), (272, 185), (252, 200), (360, 270), (700, 476), (258, 195), (400, 279), (1300, 1008), (474, 301), (1024, 597), (823, 400), (500, 375), (1000, 997), (699, 474), (768, 528), (378, 248), (270, 242), (1563, 1000), (270, 187), (800, 450), (900, 775), (642, 260), (640, 479), (600, 800), (4436, 1740), (360, 254), (1913, 2193), (1000, 999), (650, 465), (1800, 1013), (497, 351), (500, 432), (600, 400), (260, 194), (800, 544), (1200, 868), (1300, 1065), (910, 852), (1080, 1080), (350, 280), (900, 576), (800, 800), (1600, 1674), (800, 690), (1200, 706), (217, 217), (378, 378), (400, 256), (768, 1024), (1600, 1146), (1280, 790), (640, 472), (400, 265), (1116, 892), (231, 218), (342, 342), (232, 217), (293, 172), (1300, 1049), (1000, 818), (394, 297), (420, 316), (176, 322), (5504, 3096), (472, 354), (463, 360), (850, 684), (370, 251), (1536, 1536), (933, 700), (2000, 1500), (283, 240), (1536, 2048), (1200, 900), (600, 496), (754, 435), (400, 267), (1600, 1157), (900, 733), (848, 607), (1024, 576), (600, 450), (685, 427), (378, 282), (698, 400), (250, 188), (1024, 768), (800, 603), (350, 220), (350, 458), (1275, 1024), (1000, 667), (280, 326), (1094, 720), (900, 708), (900, 717), (225, 225), (163, 186), (772, 742), (275, 183), (1280, 858), (1799, 1210), (700, 1003), (2006, 1606), (266, 189), (580, 529), (378, 330), (700, 466), (1420, 1091), (198, 198), (3807, 3048), (259, 214), (640, 488), (500, 374), (150, 133), (1105, 1390), (200, 201), (273, 185), (274, 184), (498, 358), (350, 350), (236, 154), (265, 190), (294, 171), (255, 198), (739, 558), (721, 900), (1300, 973), (600, 427), (380, 250), (912, 684), (432, 324), (305, 165), (338, 232), (477, 420), (198, 255), (900, 676), (800, 534), (325, 216), (182, 277), (270, 280), (208, 241), (4530, 3006), (213, 237), (657, 460), (958, 656), (850, 565), (900, 648), (768, 523), (1100, 703), (360, 237), (1600, 1200), (4252, 2890), (561, 480), (1600, 1319), (211, 239), (320, 320), (360, 191), (230, 219), (714, 579), (1000, 1000), (700, 470), (459, 716), (236, 177), (800, 600), (900, 687), (701, 487), (564, 564), (200, 150), (1400, 1400), (1300, 959), (300, 225), (560, 560), (512, 300), (420, 290), (260, 280), (1300, 1169), (850, 850), (1280, 1175), (300, 200), (262, 280), (1080, 1440), (426, 315), (278, 181), (675, 900), (4773, 2810), (765, 900), (454, 404), (320, 205), (378, 256), (1300, 1098), (259, 195), (1000, 995), (597, 480), (800, 531), (1500, 1107), (296, 470), (850, 916), (400, 280), (268, 188), (1420, 1093), (320, 180), (1000, 961), (1280, 960), (852, 480), (1280, 1024), (600, 600), (550, 352), (1300, 981), (262, 193), (640, 480), (720, 534), (931, 974), (800, 533), (900, 675), (944, 709), (1950, 1959), (223, 167), (734, 481), (1800, 1206), (194, 259), (1300, 956), (400, 300), (177, 133), (674, 900), (474, 313), (1266, 1027), (900, 659), (450, 450), (378, 324), (640, 1138), (500, 469), (2892, 1940), (700, 460), (639, 429), (239, 211), (600, 394)}\n",
      "Unique validation image sizes: {(559, 750), (543, 543), (179, 282), (204, 247), (1280, 720), (640, 640), (900, 597), (129, 137), (2080, 1536), (1500, 1125), (511, 336), (752, 548), (259, 194), (570, 489), (1500, 1225), (267, 189), (700, 476), (684, 776), (250, 200), (1813, 1206), (270, 187), (863, 582), (600, 800), (217, 233), (652, 429), (260, 194), (800, 800), (1000, 800), (3000, 2000), (854, 1390), (1479, 1600), (684, 762), (1300, 1058), (850, 620), (800, 601), (489, 443), (450, 250), (2000, 1500), (1060, 1390), (1920, 1080), (400, 267), (250, 170), (224, 224), (350, 220), (480, 360), (225, 225), (900, 900), (275, 183), (203, 249), (274, 184), (265, 190), (721, 900), (1300, 973), (269, 187), (894, 672), (409, 530), (180, 225), (700, 550), (478, 318), (1280, 945), (735, 709), (320, 320), (329, 332), (289, 259), (1000, 1000), (960, 720), (2448, 3264), (765, 633), (800, 600), (2000, 1545), (1140, 760), (850, 567), (2733, 1822), (260, 280), (860, 635), (226, 580), (486, 500), (240, 179), (675, 900), (1300, 1089), (1024, 861), (709, 532), (220, 139), (300, 193), (220, 157), (500, 376), (664, 664), (600, 600), (500, 348), (378, 258), (612, 574), (700, 467), (640, 480), (196, 257), (780, 585), (253, 199), (237, 212), (810, 510), (640, 553), (1500, 1530), (850, 610), (699, 932), (2724, 1798), (1200, 750), (2200, 1649), (400, 300), (1300, 956), (391, 480), (600, 300)}\n",
      "Unique test image sizes: {(240, 185), (1024, 721), (400, 247), (3749, 2399), (287, 199), (1280, 720), (628, 620), (1000, 739), (604, 391), (850, 532), (865, 1225), (958, 641), (850, 797), (800, 485), (150, 150), (861, 900), (337, 449), (859, 900), (800, 640), (550, 367), (1600, 1151), (339, 339), (192, 241), (1500, 1600), (500, 375), (287, 240), (270, 288), (995, 1000), (640, 479), (417, 352), (1600, 1290), (400, 290), (600, 400), (1920, 2561), (215, 235), (4550, 2994), (900, 713), (378, 378), (1248, 920), (342, 342), (1300, 1049), (1800, 1198), (324, 254), (640, 438), (356, 142), (1600, 1157), (640, 422), (600, 450), (378, 282), (1024, 768), (189, 266), (1392, 850), (350, 220), (980, 744), (1000, 685), (1600, 1205), (275, 183), (256, 331), (220, 192), (274, 184), (506, 396), (350, 350), (721, 900), (1023, 808), (1300, 973), (1360, 1024), (682, 1024), (700, 459), (432, 324), (640, 426), (417, 244), (631, 472), (1600, 1200), (1072, 858), (1000, 1000), (1024, 756), (600, 511), (504, 504), (584, 622), (308, 211), (240, 186), (300, 225), (400, 367), (750, 498), (650, 560), (260, 280), (384, 389), (860, 635), (850, 441), (4566, 2676), (588, 386), (765, 900), (900, 600), (259, 195), (736, 711), (289, 174), (1500, 945), (1280, 960), (600, 600), (184, 274), (837, 628), (640, 480), (462, 458), (3024, 3024), (401, 320), (1272, 1004), (567, 425), (2596, 2262), (800, 691), (765, 724), (1616, 1216), (226, 223)}\n",
      "Found 360 images belonging to 3 classes.\n",
      "Found 120 images belonging to 3 classes.\n",
      "Total training images: 360\n",
      "Total validation images: 120\n",
      "Total test images: 120\n",
      "Epoch 1/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 224ms/step - accuracy: 0.3853 - loss: 1.5223 - val_accuracy: 0.3542 - val_loss: 1.1072\n",
      "Epoch 2/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2812 - loss: 1.1153 - val_accuracy: 0.3333 - val_loss: 1.0876\n",
      "Epoch 3/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 232ms/step - accuracy: 0.4046 - loss: 1.0855 - val_accuracy: 0.3229 - val_loss: 1.1971\n",
      "Epoch 4/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3438 - loss: 1.1194 - val_accuracy: 0.4167 - val_loss: 1.1158\n",
      "Epoch 5/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 206ms/step - accuracy: 0.3065 - loss: 1.1365 - val_accuracy: 0.3333 - val_loss: 1.0831\n",
      "Epoch 6/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1875 - loss: 1.0960 - val_accuracy: 0.3333 - val_loss: 1.1140\n",
      "Epoch 7/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 211ms/step - accuracy: 0.3668 - loss: 1.0909 - val_accuracy: 0.4062 - val_loss: 1.0828\n",
      "Epoch 8/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5625 - loss: 1.0756 - val_accuracy: 0.4167 - val_loss: 1.0427\n",
      "Epoch 9/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 222ms/step - accuracy: 0.3998 - loss: 1.0771 - val_accuracy: 0.3646 - val_loss: 1.1055\n",
      "Epoch 10/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2500 - loss: 1.1245 - val_accuracy: 0.4583 - val_loss: 1.0469\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.3438 - loss: 1.1155\n",
      "Validation loss: 1.098673701286316\n",
      "Validation accuracy: 0.34375\n",
      "Total test images being processed: 120\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Predictions saved to preds.txt\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Define directories\n",
    "train_dir = 'yola/split_dataset/train'\n",
    "validation_dir = 'yola/split_dataset/validation'\n",
    "test_dir = 'yola/split_dataset/test'\n",
    "test_filenames_path = 'yola/test_filenames.txt'\n",
    "\n",
    "# Check image sizes\n",
    "def check_image_sizes(directory):\n",
    "    sizes = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(('jpg', 'jpeg', 'png')):\n",
    "                img_path = os.path.join(root, file)\n",
    "                with Image.open(img_path) as img:\n",
    "                    sizes.append(img.size)\n",
    "    return sizes\n",
    "\n",
    "# Check image sizes in training, validation, and test directories\n",
    "train_sizes = check_image_sizes(train_dir)\n",
    "validation_sizes = check_image_sizes(validation_dir)\n",
    "test_sizes = check_image_sizes(test_dir)\n",
    "\n",
    "# Print unique sizes\n",
    "print(f\"Unique training image sizes: {set(train_sizes)}\")\n",
    "print(f\"Unique validation image sizes: {set(validation_sizes)}\")\n",
    "print(f\"Unique test image sizes: {set(test_sizes)}\")\n",
    "\n",
    "# Data augmentation for training and basic scaling for validation/testing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,  # Normalize pixel values to [0, 1]\n",
    "    rotation_range=20,  # Random rotations\n",
    "    width_shift_range=0.2,  # Horizontal shifts\n",
    "    height_shift_range=0.2,  # Vertical shifts\n",
    "    horizontal_flip=True  # Random horizontal flips\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "# Load datasets\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(150, 150),  # Resize images to 150x150 pixels\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(150, 150),  # Resize images to 150x150 pixels\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Debug information\n",
    "print(f\"Total training images: {train_generator.samples}\")\n",
    "print(f\"Total validation images: {validation_generator.samples}\")\n",
    "\n",
    "# Count total test images\n",
    "with open(test_filenames_path, 'r') as file:\n",
    "    test_filenames = file.read().splitlines()\n",
    "\n",
    "print(f\"Total test images: {len(test_filenames)}\")\n",
    "\n",
    "# Model definition\n",
    "model = Sequential([\n",
    "    Input(shape=(150, 150, 3)),\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(3, activation='softmax')  # Assuming 3 classes: bacilli, cocci, spirilla\n",
    "])\n",
    "\n",
    "# Compile the model with the specified learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size\n",
    ")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "validation_steps = validation_generator.samples // batch_size\n",
    "val_loss, val_accuracy = model.evaluate(validation_generator, steps=validation_steps)\n",
    "print(f\"Validation loss: {val_loss}\")\n",
    "print(f\"Validation accuracy: {val_accuracy}\")\n",
    "\n",
    "# Predict on test data\n",
    "test_images = []\n",
    "for filename in test_filenames:\n",
    "    img_path = os.path.join(test_dir, filename)\n",
    "    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(150, 150))\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    test_images.append(img_array)\n",
    "\n",
    "test_images = np.vstack(test_images)\n",
    "\n",
    "print(f\"Total test images being processed: {len(test_images)}\")\n",
    "\n",
    "# Predict on test data\n",
    "predictions = model.predict(test_images)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Save predictions to a file\n",
    "output_file = 'preds.txt'\n",
    "with open(output_file, 'w') as f:\n",
    "    for label in predicted_classes:\n",
    "        f.write(f\"{label}\\n\")\n",
    "\n",
    "print(f\"Predictions saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67531fc",
   "metadata": {},
   "source": [
    "## preprocessing but no other data augmentation such as image shifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac6da6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 360 images belonging to 3 classes.\n",
      "Found 120 images belonging to 3 classes.\n",
      "Total training images: 360\n",
      "Total validation images: 120\n",
      "Total test images: 120\n",
      "Epoch 1/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 470ms/step - accuracy: 0.3353 - loss: 1028.4517 - val_accuracy: 0.3021 - val_loss: 2.5649\n",
      "Epoch 2/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.3438 - loss: 2.5275 - val_accuracy: 0.3333 - val_loss: 4.9981\n",
      "Epoch 3/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 457ms/step - accuracy: 0.3778 - loss: 2.1420 - val_accuracy: 0.4062 - val_loss: 1.0631\n",
      "Epoch 4/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4688 - loss: 1.0724 - val_accuracy: 0.2917 - val_loss: 1.1808\n",
      "Epoch 5/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 491ms/step - accuracy: 0.4665 - loss: 1.0068 - val_accuracy: 0.4062 - val_loss: 1.0449\n",
      "Epoch 6/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5000 - loss: 0.8277 - val_accuracy: 0.3333 - val_loss: 1.1338\n",
      "Epoch 7/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 455ms/step - accuracy: 0.5104 - loss: 0.9417 - val_accuracy: 0.3542 - val_loss: 1.2010\n",
      "Epoch 8/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6250 - loss: 0.8892 - val_accuracy: 0.4167 - val_loss: 1.0708\n",
      "Epoch 9/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 465ms/step - accuracy: 0.5007 - loss: 0.8653 - val_accuracy: 0.4375 - val_loss: 1.3619\n",
      "Epoch 10/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.4375 - loss: 1.0139 - val_accuracy: 0.3750 - val_loss: 1.2946\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.4479 - loss: 1.2303\n",
      "Validation loss: 1.1804399490356445\n",
      "Validation accuracy: 0.4583333432674408\n",
      "Total test images being processed: 120\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "Predictions saved to preds.txt\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "target_size = (224, 224)  # Resize images to 224x224 pixels\n",
    "rescale_factor = 1.0 / 255  # Normalize pixel values to [0, 1]\n",
    "\n",
    "# Define directories\n",
    "train_dir = 'yola/split_dataset/train'\n",
    "validation_dir = 'yola/split_dataset/validation'\n",
    "test_dir = 'yola/split_dataset/test'\n",
    "test_filenames_path = 'yola/test_filenames.txt'\n",
    "\n",
    "# Function to preprocess images\n",
    "def preprocess_images(directory, target_size, rescale_factor):\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(('jpg', 'jpeg', 'png')):\n",
    "                img_path = os.path.join(root, file)\n",
    "                img = load_img(img_path)\n",
    "                img = img.resize(target_size)\n",
    "                img_array = img_to_array(img) * rescale_factor  # Normalize pixel values to [0, 1]\n",
    "                img = array_to_img(img_array)\n",
    "                img.save(img_path)\n",
    "\n",
    "# Preprocess images in training, validation, and test directories\n",
    "preprocess_images(train_dir, target_size, rescale_factor)\n",
    "preprocess_images(validation_dir, target_size, rescale_factor)\n",
    "preprocess_images(test_dir, target_size, rescale_factor)\n",
    "\n",
    "# Data generators for training, validation, and test datasets\n",
    "datagen = ImageDataGenerator()  # No additional preprocessing needed\n",
    "\n",
    "# Load datasets\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=target_size,  # Images are already resized to 224x224 pixels\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=target_size,  # Images are already resized to 224x224 pixels\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Debug information\n",
    "print(f\"Total training images: {train_generator.samples}\")\n",
    "print(f\"Total validation images: {validation_generator.samples}\")\n",
    "\n",
    "# Count total test images\n",
    "with open(test_filenames_path, 'r') as file:\n",
    "    test_filenames = file.read().splitlines()\n",
    "\n",
    "print(f\"Total test images: {len(test_filenames)}\")\n",
    "\n",
    "# Model definition\n",
    "model = Sequential([\n",
    "    Input(shape=(224, 224, 3)),\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(3, activation='softmax')  # Assuming 3 classes: bacilli, cocci, spirilla\n",
    "])\n",
    "\n",
    "# Compile the model with the specified learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size\n",
    ")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "validation_steps = validation_generator.samples // batch_size\n",
    "val_loss, val_accuracy = model.evaluate(validation_generator, steps=validation_steps)\n",
    "print(f\"Validation loss: {val_loss}\")\n",
    "print(f\"Validation accuracy: {val_accuracy}\")\n",
    "\n",
    "# Predict on test data\n",
    "test_images = []\n",
    "for filename in test_filenames:\n",
    "    img_path = os.path.join(test_dir, filename)\n",
    "    img = load_img(img_path, target_size=target_size)\n",
    "    img_array = img_to_array(img) * rescale_factor  # Normalize pixel values to [0, 1]\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    test_images.append(img_array)\n",
    "\n",
    "test_images = np.vstack(test_images)\n",
    "\n",
    "print(f\"Total test images being processed: {len(test_images)}\")\n",
    "\n",
    "# Predict on test data\n",
    "predictions = model.predict(test_images)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Save predictions to a file\n",
    "output_file = 'preds.txt'\n",
    "with open(output_file, 'w') as f:\n",
    "    for label in predicted_classes:\n",
    "        f.write(f\"{label}\\n\")\n",
    "\n",
    "print(f\"Predictions saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae282772",
   "metadata": {},
   "source": [
    "## more organized version of the full code, separating hyperparameters and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc6e6347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 360 images belonging to 3 classes.\n",
      "Found 120 images belonging to 3 classes.\n",
      "Total training images: 360\n",
      "Total validation images: 120\n",
      "Total test images: 120\n",
      "Epoch 1/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 481ms/step - accuracy: 0.3684 - loss: 674.5725 - val_accuracy: 0.3646 - val_loss: 6.3188\n",
      "Epoch 2/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.3438 - loss: 5.6166 - val_accuracy: 0.3333 - val_loss: 3.9543\n",
      "Epoch 3/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 454ms/step - accuracy: 0.4000 - loss: 2.0983 - val_accuracy: 0.5000 - val_loss: 1.0899\n",
      "Epoch 4/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.3750 - loss: 1.0075 - val_accuracy: 0.4583 - val_loss: 1.5674\n",
      "Epoch 5/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 465ms/step - accuracy: 0.4977 - loss: 0.9872 - val_accuracy: 0.4062 - val_loss: 1.2592\n",
      "Epoch 6/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5938 - loss: 0.8683 - val_accuracy: 0.5833 - val_loss: 1.0342\n",
      "Epoch 7/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 483ms/step - accuracy: 0.5491 - loss: 0.8782 - val_accuracy: 0.4375 - val_loss: 1.2784\n",
      "Epoch 8/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.4688 - loss: 0.9293 - val_accuracy: 0.3750 - val_loss: 1.0369\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 23:25:42.053425: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 456ms/step - accuracy: 0.6084 - loss: 0.7712 - val_accuracy: 0.4062 - val_loss: 1.3759\n",
      "Epoch 10/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6250 - loss: 0.6819 - val_accuracy: 0.5000 - val_loss: 2.2282\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.4310 - loss: 1.7876\n",
      "Validation loss: 1.7037063837051392\n",
      "Validation accuracy: 0.4791666567325592\n",
      "Total test images being processed: 120\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step\n",
      "Predictions saved to preds.txt\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Preprocessing parameters\n",
    "target_size = (224, 224)  # Resize images to 224x224 pixels\n",
    "rescale_factor = 1.0 / 255  # Normalize pixel values to [0, 1]\n",
    "\n",
    "# Define directories\n",
    "train_dir = 'yola/split_dataset/train'\n",
    "validation_dir = 'yola/split_dataset/validation'\n",
    "test_dir = 'yola/split_dataset/test'\n",
    "test_filenames_path = 'yola/test_filenames.txt'\n",
    "\n",
    "# Function to preprocess images\n",
    "def preprocess_images(directory, target_size, rescale_factor):\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(('jpg', 'jpeg', 'png')):\n",
    "                img_path = os.path.join(root, file)\n",
    "                img = load_img(img_path)\n",
    "                img = img.resize(target_size)\n",
    "                img_array = img_to_array(img) * rescale_factor  # Normalize pixel values to [0, 1]\n",
    "                img = array_to_img(img_array)\n",
    "                img.save(img_path)\n",
    "\n",
    "# Preprocess images in training, validation, and test directories\n",
    "preprocess_images(train_dir, target_size, rescale_factor)\n",
    "preprocess_images(validation_dir, target_size, rescale_factor)\n",
    "preprocess_images(test_dir, target_size, rescale_factor)\n",
    "\n",
    "# Data generators for training, validation, and test datasets\n",
    "datagen = ImageDataGenerator()  # No additional preprocessing needed\n",
    "\n",
    "# Load datasets\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=target_size,  # Images are already resized to 224x224 pixels\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=target_size,  # Images are already resized to 224x224 pixels\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Debug information\n",
    "print(f\"Total training images: {train_generator.samples}\")\n",
    "print(f\"Total validation images: {validation_generator.samples}\")\n",
    "\n",
    "# Count total test images\n",
    "with open(test_filenames_path, 'r') as file:\n",
    "    test_filenames = file.read().splitlines()\n",
    "\n",
    "print(f\"Total test images: {len(test_filenames)}\")\n",
    "\n",
    "# Model definition\n",
    "model = Sequential([\n",
    "    Input(shape=(224, 224, 3)),\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(3, activation='softmax')  # Assuming 3 classes: bacilli, cocci, spirilla\n",
    "])\n",
    "\n",
    "# Compile the model with the specified learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size\n",
    ")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "validation_steps = validation_generator.samples // batch_size\n",
    "val_loss, val_accuracy = model.evaluate(validation_generator, steps=validation_steps)\n",
    "print(f\"Validation loss: {val_loss}\")\n",
    "print(f\"Validation accuracy: {val_accuracy}\")\n",
    "\n",
    "# Predict on test data\n",
    "test_images = []\n",
    "for filename in test_filenames:\n",
    "    img_path = os.path.join(test_dir, filename)\n",
    "    img = load_img(img_path, target_size=target_size)\n",
    "    img_array = img_to_array(img) * rescale_factor  # Normalize pixel values to [0, 1]\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    test_images.append(img_array)\n",
    "\n",
    "test_images = np.vstack(test_images)\n",
    "\n",
    "print(f\"Total test images being processed: {len(test_images)}\")\n",
    "\n",
    "# Predict on test data\n",
    "predictions = model.predict(test_images)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Save predictions to a file\n",
    "output_file = 'preds.txt'\n",
    "with open(output_file, 'w') as f:\n",
    "    for label in predicted_classes:\n",
    "        f.write(f\"{label}\\n\")\n",
    "\n",
    "print(f\"Predictions saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04459cd",
   "metadata": {},
   "source": [
    "## Hyperparameters of default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29d477bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 360 images belonging to 3 classes.\n",
      "Found 120 images belonging to 3 classes.\n",
      "Total training images: 360\n",
      "Total validation images: 120\n",
      "Total test images: 120\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 501ms/step - accuracy: 0.3889 - loss: 941.4451 - val_accuracy: 0.4062 - val_loss: 2.7928\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.3229 - loss: 2.9035\n",
      "Validation loss: 2.7656784057617188\n",
      "Validation accuracy: 0.3645833432674408\n",
      "Total test images being processed: 120\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 128ms/step\n",
      "Predictions saved to preds.txt\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "epochs = 1\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Preprocessing parameters\n",
    "target_size = (224, 224)  # Resize images to 224x224 pixels\n",
    "rescale_factor = 1.0 / 255  # Normalize pixel values to [0, 1]\n",
    "\n",
    "# Define directories\n",
    "train_dir = 'yola/split_dataset/train'\n",
    "validation_dir = 'yola/split_dataset/validation'\n",
    "test_dir = 'yola/split_dataset/test'\n",
    "test_filenames_path = 'yola/test_filenames.txt'\n",
    "\n",
    "# Function to preprocess images\n",
    "def preprocess_images(directory, target_size, rescale_factor):\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(('jpg', 'jpeg', 'png')):\n",
    "                img_path = os.path.join(root, file)\n",
    "                img = load_img(img_path)\n",
    "                img = img.resize(target_size)\n",
    "                img_array = img_to_array(img) * rescale_factor  # Normalize pixel values to [0, 1]\n",
    "                img = array_to_img(img_array)\n",
    "                img.save(img_path)\n",
    "\n",
    "# Preprocess images in training, validation, and test directories\n",
    "preprocess_images(train_dir, target_size, rescale_factor)\n",
    "preprocess_images(validation_dir, target_size, rescale_factor)\n",
    "preprocess_images(test_dir, target_size, rescale_factor)\n",
    "\n",
    "# Data generators for training, validation, and test datasets\n",
    "datagen = ImageDataGenerator()  # No additional preprocessing needed\n",
    "\n",
    "# Load datasets\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=target_size,  # Images are already resized to 224x224 pixels\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=target_size,  # Images are already resized to 224x224 pixels\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Debug information\n",
    "print(f\"Total training images: {train_generator.samples}\")\n",
    "print(f\"Total validation images: {validation_generator.samples}\")\n",
    "\n",
    "# Count total test images\n",
    "with open(test_filenames_path, 'r') as file:\n",
    "    test_filenames = file.read().splitlines()\n",
    "\n",
    "print(f\"Total test images: {len(test_filenames)}\")\n",
    "\n",
    "# Model definition\n",
    "model = Sequential([\n",
    "    Input(shape=(224, 224, 3)),\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(3, activation='softmax')  # Assuming 3 classes: bacilli, cocci, spirilla\n",
    "])\n",
    "\n",
    "# Compile the model with the specified learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size\n",
    ")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "validation_steps = validation_generator.samples // batch_size\n",
    "val_loss, val_accuracy = model.evaluate(validation_generator, steps=validation_steps)\n",
    "print(f\"Validation loss: {val_loss}\")\n",
    "print(f\"Validation accuracy: {val_accuracy}\")\n",
    "\n",
    "# Predict on test data\n",
    "test_images = []\n",
    "for filename in test_filenames:\n",
    "    img_path = os.path.join(test_dir, filename)\n",
    "    img = load_img(img_path, target_size=target_size)\n",
    "    img_array = img_to_array(img) * rescale_factor  # Normalize pixel values to [0, 1]\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    test_images.append(img_array)\n",
    "\n",
    "test_images = np.vstack(test_images)\n",
    "\n",
    "print(f\"Total test images being processed: {len(test_images)}\")\n",
    "\n",
    "# Predict on test data\n",
    "predictions = model.predict(test_images)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Save predictions to a file\n",
    "output_file = 'preds.txt'\n",
    "with open(output_file, 'w') as f:\n",
    "    for label in predicted_classes:\n",
    "        f.write(f\"{label}\\n\")\n",
    "\n",
    "print(f\"Predictions saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51784f8",
   "metadata": {},
   "source": [
    "## Hyperparameters: Epochs 8, Learning Rate 0.0001, Preprocessing of 256x256 image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f328b252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 360 images belonging to 3 classes.\n",
      "Found 120 images belonging to 3 classes.\n",
      "Total training images: 360\n",
      "Total validation images: 120\n",
      "Total test images: 120\n",
      "Epoch 1/8\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 700ms/step - accuracy: 0.3294 - loss: 268.3039 - val_accuracy: 0.3958 - val_loss: 39.2884\n",
      "Epoch 2/8\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.4375 - loss: 70.2922 - val_accuracy: 0.2917 - val_loss: 17.3065\n",
      "Epoch 3/8\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 649ms/step - accuracy: 0.3468 - loss: 46.5477 - val_accuracy: 0.3438 - val_loss: 15.9998\n",
      "Epoch 4/8\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.2500 - loss: 29.8877 - val_accuracy: 0.3750 - val_loss: 14.1589\n",
      "Epoch 5/8\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 596ms/step - accuracy: 0.4270 - loss: 15.6166 - val_accuracy: 0.4688 - val_loss: 2.4501\n",
      "Epoch 6/8\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.4688 - loss: 3.9952 - val_accuracy: 0.4583 - val_loss: 2.3887\n",
      "Epoch 7/8\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 602ms/step - accuracy: 0.3943 - loss: 3.2554 - val_accuracy: 0.4688 - val_loss: 0.9898\n",
      "Epoch 8/8\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.4062 - loss: 1.4592 - val_accuracy: 0.3750 - val_loss: 1.2977\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.3451 - loss: 1.4146\n",
      "Validation loss: 1.430116057395935\n",
      "Validation accuracy: 0.3541666567325592\n",
      "Total test images being processed: 120\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 144ms/step\n",
      "Predictions saved to preds.txt\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "epochs = 8\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Preprocessing parameters\n",
    "target_size = (256, 256)  # Resize images to 256x256 pixels\n",
    "rescale_factor = 1.0 / 255  # Normalize pixel values to [0, 1]\n",
    "\n",
    "# Define directories\n",
    "train_dir = 'yola/split_dataset/train'\n",
    "validation_dir = 'yola/split_dataset/validation'\n",
    "test_dir = 'yola/split_dataset/test'\n",
    "test_filenames_path = 'yola/test_filenames.txt'\n",
    "\n",
    "# Function to preprocess images\n",
    "def preprocess_images(directory, target_size, rescale_factor):\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(('jpg', 'jpeg', 'png')):\n",
    "                img_path = os.path.join(root, file)\n",
    "                img = load_img(img_path)\n",
    "                img = img.resize(target_size)\n",
    "                img_array = img_to_array(img) * rescale_factor  # Normalize pixel values to [0, 1]\n",
    "                img = array_to_img(img_array)\n",
    "                img.save(img_path)\n",
    "\n",
    "# Preprocess images in training, validation, and test directories\n",
    "preprocess_images(train_dir, target_size, rescale_factor)\n",
    "preprocess_images(validation_dir, target_size, rescale_factor)\n",
    "preprocess_images(test_dir, target_size, rescale_factor)\n",
    "\n",
    "# Data generators for training, validation, and test datasets\n",
    "datagen = ImageDataGenerator()  # No additional preprocessing needed\n",
    "\n",
    "# Load datasets\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=target_size,  # Images are already resized to 256x256 pixels\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=target_size,  # Images are already resized to 256x256 pixels\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Debug information\n",
    "print(f\"Total training images: {train_generator.samples}\")\n",
    "print(f\"Total validation images: {validation_generator.samples}\")\n",
    "\n",
    "# Count total test images\n",
    "with open(test_filenames_path, 'r') as file:\n",
    "    test_filenames = file.read().splitlines()\n",
    "\n",
    "print(f\"Total test images: {len(test_filenames)}\")\n",
    "\n",
    "# Model definition\n",
    "model = Sequential([\n",
    "    Input(shape=(256, 256, 3)),\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(3, activation='softmax')  # Assuming 3 classes: bacilli, cocci, spirilla\n",
    "])\n",
    "\n",
    "# Compile the model with the specified learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size\n",
    ")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "validation_steps = validation_generator.samples // batch_size\n",
    "val_loss, val_accuracy = model.evaluate(validation_generator, steps=validation_steps)\n",
    "print(f\"Validation loss: {val_loss}\")\n",
    "print(f\"Validation accuracy: {val_accuracy}\")\n",
    "\n",
    "# Predict on test data\n",
    "test_images = []\n",
    "for filename in test_filenames:\n",
    "    img_path = os.path.join(test_dir, filename)\n",
    "    img = load_img(img_path, target_size=target_size)\n",
    "    img_array = img_to_array(img) * rescale_factor  # Normalize pixel values to [0, 1]\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    test_images.append(img_array)\n",
    "\n",
    "test_images = np.vstack(test_images)\n",
    "\n",
    "print(f\"Total test images being processed: {len(test_images)}\")\n",
    "\n",
    "# Predict on test data\n",
    "predictions = model.predict(test_images)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Save predictions to a file\n",
    "output_file = 'preds.txt'\n",
    "with open(output_file, 'w') as f:\n",
    "    for label in predicted_classes:\n",
    "        f.write(f\"{label}\\n\")\n",
    "\n",
    "print(f\"Predictions saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e33f1f",
   "metadata": {},
   "source": [
    "## Hyperparameters of Epochs 12, Learning Rate 0.001, image size of 256x256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd029c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 360 images belonging to 3 classes.\n",
      "Found 120 images belonging to 3 classes.\n",
      "Total training images: 360\n",
      "Total validation images: 120\n",
      "Total test images: 120\n",
      "Epoch 1/12\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 621ms/step - accuracy: 0.3223 - loss: 2460.7139 - val_accuracy: 0.3021 - val_loss: 3.4080\n",
      "Epoch 2/12\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.3438 - loss: 2.5897 - val_accuracy: 0.5000 - val_loss: 4.0391\n",
      "Epoch 3/12\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 670ms/step - accuracy: 0.3855 - loss: 1.6794 - val_accuracy: 0.3854 - val_loss: 1.1415\n",
      "Epoch 4/12\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5312 - loss: 1.0409 - val_accuracy: 0.5000 - val_loss: 0.9794\n",
      "Epoch 5/12\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 597ms/step - accuracy: 0.4845 - loss: 0.9771 - val_accuracy: 0.4062 - val_loss: 1.0174\n",
      "Epoch 6/12\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.4688 - loss: 0.9587 - val_accuracy: 0.2917 - val_loss: 1.1020\n",
      "Epoch 7/12\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 584ms/step - accuracy: 0.4955 - loss: 0.9177 - val_accuracy: 0.4271 - val_loss: 1.2658\n",
      "Epoch 8/12\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.4688 - loss: 1.0651 - val_accuracy: 0.3750 - val_loss: 1.0239\n",
      "Epoch 9/12\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 603ms/step - accuracy: 0.5526 - loss: 0.8828 - val_accuracy: 0.4375 - val_loss: 1.3747\n",
      "Epoch 10/12\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6562 - loss: 0.6820 - val_accuracy: 0.4167 - val_loss: 1.2070\n",
      "Epoch 11/12\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 576ms/step - accuracy: 0.5805 - loss: 0.7168 - val_accuracy: 0.4375 - val_loss: 1.5129\n",
      "Epoch 12/12\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6250 - loss: 0.7235 - val_accuracy: 0.5000 - val_loss: 2.5293\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.4674 - loss: 1.1941\n",
      "Validation loss: 1.239443302154541\n",
      "Validation accuracy: 0.4583333432674408\n",
      "Total test images being processed: 120\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 144ms/step\n",
      "Predictions saved to preds.txt\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "epochs = 12\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Preprocessing parameters\n",
    "target_size = (256, 256)  # Resize images to 256x256 pixels\n",
    "rescale_factor = 1.0 / 255  # Normalize pixel values to [0, 1]\n",
    "\n",
    "# Define directories\n",
    "train_dir = 'yola/split_dataset/train'\n",
    "validation_dir = 'yola/split_dataset/validation'\n",
    "test_dir = 'yola/split_dataset/test'\n",
    "test_filenames_path = 'yola/test_filenames.txt'\n",
    "\n",
    "# Function to preprocess images\n",
    "def preprocess_images(directory, target_size, rescale_factor):\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(('jpg', 'jpeg', 'png')):\n",
    "                img_path = os.path.join(root, file)\n",
    "                img = load_img(img_path)\n",
    "                img = img.resize(target_size)\n",
    "                img_array = img_to_array(img) * rescale_factor  # Normalize pixel values to [0, 1]\n",
    "                img = array_to_img(img_array)\n",
    "                img.save(img_path)\n",
    "\n",
    "# Preprocess images in training, validation, and test directories\n",
    "preprocess_images(train_dir, target_size, rescale_factor)\n",
    "preprocess_images(validation_dir, target_size, rescale_factor)\n",
    "preprocess_images(test_dir, target_size, rescale_factor)\n",
    "\n",
    "# Data generators for training, validation, and test datasets\n",
    "datagen = ImageDataGenerator()  # No additional preprocessing needed\n",
    "\n",
    "# Load datasets\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=target_size,  # Images are already resized to 256x256 pixels\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=target_size,  # Images are already resized to 256x256 pixels\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Debug information\n",
    "print(f\"Total training images: {train_generator.samples}\")\n",
    "print(f\"Total validation images: {validation_generator.samples}\")\n",
    "\n",
    "# Count total test images\n",
    "with open(test_filenames_path, 'r') as file:\n",
    "    test_filenames = file.read().splitlines()\n",
    "\n",
    "print(f\"Total test images: {len(test_filenames)}\")\n",
    "\n",
    "# Model definition\n",
    "model = Sequential([\n",
    "    Input(shape=(256, 256, 3)),\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(3, activation='softmax')  # Assuming 3 classes: bacilli, cocci, spirilla\n",
    "])\n",
    "\n",
    "# Compile the model with the specified learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size\n",
    ")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "validation_steps = validation_generator.samples // batch_size\n",
    "val_loss, val_accuracy = model.evaluate(validation_generator, steps=validation_steps)\n",
    "print(f\"Validation loss: {val_loss}\")\n",
    "print(f\"Validation accuracy: {val_accuracy}\")\n",
    "\n",
    "# Predict on test data\n",
    "test_images = []\n",
    "for filename in test_filenames:\n",
    "    img_path = os.path.join(test_dir, filename)\n",
    "    img = load_img(img_path, target_size=target_size)\n",
    "    img_array = img_to_array(img) * rescale_factor  # Normalize pixel values to [0, 1]\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    test_images.append(img_array)\n",
    "\n",
    "test_images = np.vstack(test_images)\n",
    "\n",
    "print(f\"Total test images being processed: {len(test_images)}\")\n",
    "\n",
    "# Predict on test data\n",
    "predictions = model.predict(test_images)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Save predictions to a file\n",
    "output_file = 'preds.txt'\n",
    "with open(output_file, 'w') as f:\n",
    "    for label in predicted_classes:\n",
    "        f.write(f\"{label}\\n\")\n",
    "\n",
    "print(f\"Predictions saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7d3331",
   "metadata": {},
   "source": [
    "## keras tuner, similar to gridsearchcv for hyperparameter tuning of keras models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce69bf2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 00m 58s]\n",
      "val_accuracy: 0.42500001192092896\n",
      "\n",
      "Best val_accuracy So Far: 0.5666666626930237\n",
      "Total elapsed time: 00h 04m 50s\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 132ms/step - accuracy: 0.3690 - loss: 36.8330\n",
      "Validation loss: 39.39779281616211\n",
      "Validation accuracy: 0.34166666865348816\n",
      "Total test images being processed: 120\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 133ms/step\n",
      "Predictions saved to preds.txt\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "epochs = 8\n",
    "\n",
    "# Preprocessing parameters\n",
    "target_size = (256, 256)  # Resize images to 256x256 pixels\n",
    "rescale_factor = 1.0 / 255  # Normalize pixel values to [0, 1]\n",
    "\n",
    "# Define directories\n",
    "train_dir = 'yola/split_dataset/train'\n",
    "validation_dir = 'yola/split_dataset/validation'\n",
    "test_dir = 'yola/split_dataset/test'\n",
    "test_filenames_path = 'yola/test_filenames.txt'\n",
    "\n",
    "# Function to preprocess images\n",
    "def preprocess_images(directory, target_size, rescale_factor):\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(('jpg', 'jpeg', 'png')):\n",
    "                img_path = os.path.join(root, file)\n",
    "                img = load_img(img_path)\n",
    "                img = img.resize(target_size)\n",
    "                img_array = img_to_array(img) * rescale_factor  # Normalize pixel values to [0, 1]\n",
    "                img = array_to_img(img_array)\n",
    "                img.save(img_path)\n",
    "\n",
    "# Preprocess images in training, validation, and test directories\n",
    "preprocess_images(train_dir, target_size, rescale_factor)\n",
    "preprocess_images(validation_dir, target_size, rescale_factor)\n",
    "preprocess_images(test_dir, target_size, rescale_factor)\n",
    "\n",
    "# Data generators for training, validation, and test datasets\n",
    "datagen = ImageDataGenerator()  # No additional preprocessing needed\n",
    "\n",
    "# Load datasets\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=target_size,  # Images are already resized to 256x256 pixels\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=target_size,  # Images are already resized to 256x256 pixels\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Debug information\n",
    "print(f\"Total training images: {train_generator.samples}\")\n",
    "print(f\"Total validation images: {validation_generator.samples}\")\n",
    "\n",
    "# Function to create the model\n",
    "def build_model(hp):\n",
    "    model = Sequential([\n",
    "        Input(shape=(256, 256, 3)),\n",
    "        Conv2D(32, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(3, activation='softmax')  # Assuming 3 classes: bacilli, cocci, spirilla\n",
    "    ])\n",
    "    learning_rate = hp.Choice('learning_rate', values=[0.1, 0.01, 0.001, 0.0001, 0.00001])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initialize the tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=1,\n",
    "    directory='my_dir',\n",
    "    project_name='hyperparameter_tuning'\n",
    ")\n",
    "\n",
    "# Perform hyperparameter search\n",
    "tuner.search(train_generator, epochs=epochs, validation_data=validation_generator)\n",
    "\n",
    "# Get the best hyperparameters and model\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_loss, val_accuracy = best_model.evaluate(validation_generator)\n",
    "print(f\"Validation loss: {val_loss}\")\n",
    "print(f\"Validation accuracy: {val_accuracy}\")\n",
    "\n",
    "# Predict on test data\n",
    "test_images = []\n",
    "for filename in test_filenames:\n",
    "    img_path = os.path.join(test_dir, filename)\n",
    "    img = load_img(img_path, target_size=target_size)\n",
    "    img_array = img_to_array(img) * rescale_factor  # Normalize pixel values to [0, 1]\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    test_images.append(img_array)\n",
    "\n",
    "test_images = np.vstack(test_images)\n",
    "\n",
    "print(f\"Total test images being processed: {len(test_images)}\")\n",
    "\n",
    "# Predict on test data\n",
    "predictions = best_model.predict(test_images)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Save predictions to a file\n",
    "output_file = 'preds.txt'\n",
    "with open(output_file, 'w') as f:\n",
    "    for label in predicted_classes:\n",
    "        f.write(f\"{label}\\n\")\n",
    "\n",
    "print(f\"Predictions saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281b7b85",
   "metadata": {},
   "source": [
    "# Full Code for Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424af790",
   "metadata": {},
   "source": [
    "Here are full code views for model 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32210373",
   "metadata": {},
   "source": [
    "## basic version (MobileNetV2 Transfer Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee01c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 360 images belonging to 3 classes.\n",
      "Found 120 images belonging to 3 classes.\n",
      "Total training images: 360\n",
      "Total validation images: 120\n",
      "Epoch 1/8\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 306ms/step - accuracy: 0.4017 - loss: 1.4385 - val_accuracy: 0.5521 - val_loss: 0.9222\n",
      "Epoch 2/8\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5938 - loss: 0.8897 - val_accuracy: 0.5833 - val_loss: 0.9567\n",
      "Epoch 3/8\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 291ms/step - accuracy: 0.5820 - loss: 0.8952 - val_accuracy: 0.6875 - val_loss: 0.7946\n",
      "Epoch 4/8\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6250 - loss: 0.8169 - val_accuracy: 0.7083 - val_loss: 0.7330\n",
      "Epoch 5/8\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 265ms/step - accuracy: 0.6713 - loss: 0.7304 - val_accuracy: 0.6979 - val_loss: 0.7609\n",
      "Epoch 6/8\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5312 - loss: 0.9334 - val_accuracy: 0.7083 - val_loss: 0.6134\n",
      "Epoch 7/8\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 258ms/step - accuracy: 0.7422 - loss: 0.5635 - val_accuracy: 0.7292 - val_loss: 0.6662\n",
      "Epoch 8/8\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7812 - loss: 0.6246 - val_accuracy: 0.7083 - val_loss: 0.7930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 12:43:37.812883: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 206ms/step - accuracy: 0.6940 - loss: 0.7892\n",
      "Validation loss: 0.7374851107597351\n",
      "Validation accuracy: 0.7083333134651184\n",
      "Total test images being processed: 120\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 296ms/step\n",
      "Predictions saved to preds.txt\n"
     ]
    }
   ],
   "source": [
    "# Model 2 full code\n",
    "# We are using MobileNetV2 as the base model\n",
    "# This is a Transfer Learning approach\n",
    "# It is from the Keras module like Sequential was for CNN\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32 \n",
    "epochs = 8\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Preprocessing parameters\n",
    "target_size = (256, 256)  # Resize images to 256x256 pixels\n",
    "\n",
    "# Define directories\n",
    "train_dir = 'yola/split_dataset/train'\n",
    "validation_dir = 'yola/split_dataset/validation'\n",
    "test_dir = 'yola/split_dataset/test'\n",
    "test_filenames_path = 'yola/test_filenames.txt'\n",
    "\n",
    "# Function to preprocess images\n",
    "def preprocess_images(directory, target_size):\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(('jpg', 'jpeg', 'png')):\n",
    "                img_path = os.path.join(root, file)\n",
    "                img = load_img(img_path)\n",
    "                img = img.resize(target_size)\n",
    "                img_array = img_to_array(img)\n",
    "                img = array_to_img(img_array)\n",
    "                img.save(img_path)\n",
    "\n",
    "# Preprocess images in training, validation, and test directories\n",
    "preprocess_images(train_dir, target_size)\n",
    "preprocess_images(validation_dir, target_size)\n",
    "preprocess_images(test_dir, target_size)\n",
    "\n",
    "# Data generators for training, validation, and test datasets\n",
    "datagen = ImageDataGenerator(preprocessing_function=preprocess_input)  # Use MobileNetV2 preprocessing\n",
    "\n",
    "# Load datasets\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=target_size,  # Images are already resized to 256x256 pixels\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=target_size,  # Images are already resized to 256x256 pixels\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Debug information\n",
    "print(f\"Total training images: {train_generator.samples}\")\n",
    "print(f\"Total validation images: {validation_generator.samples}\")\n",
    "\n",
    "# Load the pre-trained MobileNetV2 model from local weights\n",
    "weights_path = './models/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5'\n",
    "base_model = MobileNetV2(input_shape=(256, 256, 3), include_top=False, weights=None)\n",
    "base_model.load_weights(weights_path)\n",
    "base_model.trainable = False  # Freeze the base model\n",
    "\n",
    "# Add custom classification layers on top of the base model\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(3, activation='softmax')(x)  # Assuming 3 classes: bacilli, cocci, spirilla\n",
    "\n",
    "# Create the full model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model with the specified learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size\n",
    ")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "validation_steps = validation_generator.samples // batch_size\n",
    "val_loss, val_accuracy = model.evaluate(validation_generator, steps=validation_steps)\n",
    "print(f\"Validation loss: {val_loss}\")\n",
    "print(f\"Validation accuracy: {val_accuracy}\")\n",
    "\n",
    "# Load test filenames\n",
    "with open(test_filenames_path, 'r') as file:\n",
    "    test_filenames = file.read().splitlines()\n",
    "\n",
    "# Predict on test data\n",
    "test_images = []\n",
    "for filename in test_filenames:\n",
    "    img_path = os.path.join(test_dir, filename)\n",
    "    img = load_img(img_path, target_size=target_size)\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = preprocess_input(img_array)  # Use MobileNetV2 preprocessing\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    test_images.append(img_array)\n",
    "\n",
    "test_images = np.vstack(test_images)\n",
    "\n",
    "print(f\"Total test images being processed: {len(test_images)}\")\n",
    "\n",
    "# Predict on test data\n",
    "predictions = model.predict(test_images)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Save predictions to a file\n",
    "output_file = 'preds.txt'\n",
    "with open(output_file, 'w') as f:\n",
    "    for label in predicted_classes:\n",
    "        f.write(f\"{label}\\n\")\n",
    "\n",
    "print(f\"Predictions saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df80914e",
   "metadata": {},
   "source": [
    "## tuning, modified hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6602fa5",
   "metadata": {},
   "source": [
    "See the code comments for the hyperparameters section for the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8df8a49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 360 images belonging to 3 classes.\n",
      "Found 120 images belonging to 3 classes.\n",
      "Total training images: 360\n",
      "Total validation images: 120\n",
      "Epoch 1/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 156ms/step - accuracy: 0.5565 - loss: 1.1182 - val_accuracy: 0.6786 - val_loss: 0.9726\n",
      "Epoch 2/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5625 - loss: 1.2289 - val_accuracy: 0.5000 - val_loss: 1.5564\n",
      "Epoch 3/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 140ms/step - accuracy: 0.8052 - loss: 0.6497 - val_accuracy: 0.7500 - val_loss: 0.7767\n",
      "Epoch 4/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9375 - loss: 0.1717 - val_accuracy: 0.7500 - val_loss: 0.3860\n",
      "Epoch 5/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 134ms/step - accuracy: 0.8896 - loss: 0.3584 - val_accuracy: 0.7946 - val_loss: 0.7090\n",
      "Epoch 6/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0762 - val_accuracy: 0.8750 - val_loss: 0.2416\n",
      "Epoch 7/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 135ms/step - accuracy: 0.9137 - loss: 0.2035 - val_accuracy: 0.7321 - val_loss: 0.8782\n",
      "Epoch 8/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8750 - loss: 0.2033 - val_accuracy: 0.6250 - val_loss: 1.1770\n",
      "Epoch 9/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 137ms/step - accuracy: 0.9052 - loss: 0.2244 - val_accuracy: 0.7321 - val_loss: 0.8082\n",
      "Epoch 10/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0728 - val_accuracy: 0.6250 - val_loss: 0.7609\n",
      "Epoch 11/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 131ms/step - accuracy: 0.9606 - loss: 0.1136 - val_accuracy: 0.7768 - val_loss: 0.7360\n",
      "Epoch 12/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0450 - val_accuracy: 0.5000 - val_loss: 1.6258\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - accuracy: 0.7278 - loss: 0.9217\n",
      "Validation loss: 0.8140392303466797\n",
      "Validation accuracy: 0.7678571343421936\n",
      "Total test images being processed: 120\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 276ms/step\n",
      "Predictions saved to preds.txt\n"
     ]
    }
   ],
   "source": [
    "# Model 2 full code\n",
    "# We are using MobileNetV2 as the base model\n",
    "# This is a Transfer Learning approach\n",
    "# It is from the Keras module like Sequential was for CNN\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 16 # from 32 to 16\n",
    "epochs = 12 # from 8 to 12\n",
    "learning_rate = 0.001 # from 0.0001 to 0.001\n",
    "\n",
    "# Preprocessing parameters\n",
    "target_size = (256, 256)  # Resize images to 256x256 pixels\n",
    "\n",
    "# Define directories\n",
    "train_dir = 'yola/split_dataset/train'\n",
    "validation_dir = 'yola/split_dataset/validation'\n",
    "test_dir = 'yola/split_dataset/test'\n",
    "test_filenames_path = 'yola/test_filenames.txt'\n",
    "\n",
    "# Function to preprocess images\n",
    "def preprocess_images(directory, target_size):\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(('jpg', 'jpeg', 'png')):\n",
    "                img_path = os.path.join(root, file)\n",
    "                img = load_img(img_path)\n",
    "                img = img.resize(target_size)\n",
    "                img_array = img_to_array(img)\n",
    "                img = array_to_img(img_array)\n",
    "                img.save(img_path)\n",
    "\n",
    "# Preprocess images in training, validation, and test directories\n",
    "preprocess_images(train_dir, target_size)\n",
    "preprocess_images(validation_dir, target_size)\n",
    "preprocess_images(test_dir, target_size)\n",
    "\n",
    "# Data generators for training, validation, and test datasets\n",
    "datagen = ImageDataGenerator(preprocessing_function=preprocess_input)  # Use MobileNetV2 preprocessing\n",
    "\n",
    "# Load datasets\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=target_size,  # Images are already resized to 256x256 pixels\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=target_size,  # Images are already resized to 256x256 pixels\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Debug information\n",
    "print(f\"Total training images: {train_generator.samples}\")\n",
    "print(f\"Total validation images: {validation_generator.samples}\")\n",
    "\n",
    "# Load the pre-trained MobileNetV2 model from local weights\n",
    "weights_path = './models/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5'\n",
    "base_model = MobileNetV2(input_shape=(256, 256, 3), include_top=False, weights=None)\n",
    "base_model.load_weights(weights_path)\n",
    "base_model.trainable = False  # Freeze the base model\n",
    "\n",
    "# Add custom classification layers on top of the base model\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(3, activation='softmax')(x)  # Assuming 3 classes: bacilli, cocci, spirilla\n",
    "\n",
    "# Create the full model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model with the specified learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size\n",
    ")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "validation_steps = validation_generator.samples // batch_size\n",
    "val_loss, val_accuracy = model.evaluate(validation_generator, steps=validation_steps)\n",
    "print(f\"Validation loss: {val_loss}\")\n",
    "print(f\"Validation accuracy: {val_accuracy}\")\n",
    "\n",
    "# Load test filenames\n",
    "with open(test_filenames_path, 'r') as file:\n",
    "    test_filenames = file.read().splitlines()\n",
    "\n",
    "# Predict on test data\n",
    "test_images = []\n",
    "for filename in test_filenames:\n",
    "    img_path = os.path.join(test_dir, filename)\n",
    "    img = load_img(img_path, target_size=target_size)\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = preprocess_input(img_array)  # Use MobileNetV2 preprocessing\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    test_images.append(img_array)\n",
    "\n",
    "test_images = np.vstack(test_images)\n",
    "\n",
    "print(f\"Total test images being processed: {len(test_images)}\")\n",
    "\n",
    "# Predict on test data\n",
    "predictions = model.predict(test_images)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Save predictions to a file\n",
    "output_file = 'preds.txt'\n",
    "with open(output_file, 'w') as f:\n",
    "    for label in predicted_classes:\n",
    "        f.write(f\"{label}\\n\")\n",
    "\n",
    "print(f\"Predictions saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f10e18a",
   "metadata": {},
   "source": [
    "Better results.  From .70 to .78 approximately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6f0f42",
   "metadata": {},
   "source": [
    "## tuning, default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b0282be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 360 images belonging to 3 classes.\n",
      "Found 120 images belonging to 3 classes.\n",
      "Total training images: 360\n",
      "Total validation images: 120\n",
      "Epoch 1/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - accuracy: 0.5447 - loss: 1.2104 - val_accuracy: 0.6250 - val_loss: 0.9523\n",
      "Epoch 2/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7500 - loss: 0.4332 - val_accuracy: 0.2500 - val_loss: 2.9400\n",
      "Epoch 3/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 138ms/step - accuracy: 0.8032 - loss: 0.5379 - val_accuracy: 0.7054 - val_loss: 0.8313\n",
      "Epoch 4/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.1358 - val_accuracy: 0.8750 - val_loss: 0.6350\n",
      "Epoch 5/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 148ms/step - accuracy: 0.8723 - loss: 0.3594 - val_accuracy: 0.7054 - val_loss: 0.7647\n",
      "Epoch 6/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.1183 - val_accuracy: 0.8750 - val_loss: 0.7213\n",
      "Epoch 7/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 138ms/step - accuracy: 0.9026 - loss: 0.2488 - val_accuracy: 0.7768 - val_loss: 0.9000\n",
      "Epoch 8/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8750 - loss: 0.2084 - val_accuracy: 0.8750 - val_loss: 0.2476\n",
      "Epoch 9/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 138ms/step - accuracy: 0.9543 - loss: 0.1720 - val_accuracy: 0.7589 - val_loss: 0.7790\n",
      "Epoch 10/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0448 - val_accuracy: 0.5000 - val_loss: 1.3815\n",
      "Epoch 11/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 144ms/step - accuracy: 0.9636 - loss: 0.1193 - val_accuracy: 0.7232 - val_loss: 0.9194\n",
      "Epoch 12/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8125 - loss: 0.2414 - val_accuracy: 0.7500 - val_loss: 0.8678\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 104ms/step - accuracy: 0.7718 - loss: 0.8195\n",
      "Validation loss: 0.8449206948280334\n",
      "Validation accuracy: 0.7589285969734192\n",
      "Total test images being processed: 120\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 285ms/step\n",
      "Predictions saved to preds.txt\n"
     ]
    }
   ],
   "source": [
    "# Model 2 full code\n",
    "# We are using MobileNetV2 as the base model\n",
    "# This is a Transfer Learning approach\n",
    "# It is from the Keras module like Sequential was for CNN\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 16 # from 32 to 16\n",
    "epochs = 12 # from 8 to 12\n",
    "learning_rate = 0.001 # from 0.0001 to 0.001\n",
    "\n",
    "# Preprocessing parameters\n",
    "target_size = (256, 256)  # Resize images to 256x256 pixels\n",
    "\n",
    "# Define directories\n",
    "train_dir = 'yola/split_dataset/train'\n",
    "validation_dir = 'yola/split_dataset/validation'\n",
    "test_dir = 'yola/split_dataset/test'\n",
    "test_filenames_path = 'yola/test_filenames.txt'\n",
    "\n",
    "# Function to preprocess images\n",
    "def preprocess_images(directory, target_size):\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(('jpg', 'jpeg', 'png')):\n",
    "                img_path = os.path.join(root, file)\n",
    "                img = load_img(img_path)\n",
    "                img = img.resize(target_size)\n",
    "                img_array = img_to_array(img)\n",
    "                img = array_to_img(img_array)\n",
    "                img.save(img_path)\n",
    "\n",
    "# Preprocess images in training, validation, and test directories\n",
    "preprocess_images(train_dir, target_size)\n",
    "preprocess_images(validation_dir, target_size)\n",
    "preprocess_images(test_dir, target_size)\n",
    "\n",
    "# Data generators for training, validation, and test datasets\n",
    "datagen = ImageDataGenerator(preprocessing_function=preprocess_input)  # Use MobileNetV2 preprocessing\n",
    "\n",
    "# Load datasets\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=target_size,  # Images are already resized to 256x256 pixels\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=target_size,  # Images are already resized to 256x256 pixels\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Debug information\n",
    "print(f\"Total training images: {train_generator.samples}\")\n",
    "print(f\"Total validation images: {validation_generator.samples}\")\n",
    "\n",
    "# Load the pre-trained MobileNetV2 model from local weights\n",
    "weights_path = './models/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5'\n",
    "base_model = MobileNetV2(input_shape=(256, 256, 3), include_top=False, weights=None)\n",
    "base_model.load_weights(weights_path)\n",
    "base_model.trainable = False  # Freeze the base model\n",
    "\n",
    "# Add custom classification layers on top of the base model\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(3, activation='softmax')(x)  # Assuming 3 classes: bacilli, cocci, spirilla\n",
    "\n",
    "# Create the full model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model with the specified learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size\n",
    ")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "validation_steps = validation_generator.samples // batch_size\n",
    "val_loss, val_accuracy = model.evaluate(validation_generator, steps=validation_steps)\n",
    "print(f\"Validation loss: {val_loss}\")\n",
    "print(f\"Validation accuracy: {val_accuracy}\")\n",
    "\n",
    "# Load test filenames\n",
    "with open(test_filenames_path, 'r') as file:\n",
    "    test_filenames = file.read().splitlines()\n",
    "\n",
    "# Predict on test data\n",
    "test_images = []\n",
    "for filename in test_filenames:\n",
    "    img_path = os.path.join(test_dir, filename)\n",
    "    img = load_img(img_path, target_size=target_size)\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = preprocess_input(img_array)  # Use MobileNetV2 preprocessing\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    test_images.append(img_array)\n",
    "\n",
    "test_images = np.vstack(test_images)\n",
    "\n",
    "print(f\"Total test images being processed: {len(test_images)}\")\n",
    "\n",
    "# Predict on test data\n",
    "predictions = model.predict(test_images)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Save predictions to a file\n",
    "output_file = 'preds.txt'\n",
    "with open(output_file, 'w') as f:\n",
    "    for label in predicted_classes:\n",
    "        f.write(f\"{label}\\n\")\n",
    "\n",
    "print(f\"Predictions saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d78c20",
   "metadata": {},
   "source": [
    "## tuning, image size to 224x224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70a8019a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 360 images belonging to 3 classes.\n",
      "Found 120 images belonging to 3 classes.\n",
      "Total training images: 360\n",
      "Total validation images: 120\n",
      "Epoch 1/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 123ms/step - accuracy: 0.5486 - loss: 1.4315 - val_accuracy: 0.7679 - val_loss: 0.7014\n",
      "Epoch 2/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5000 - loss: 1.3821 - val_accuracy: 0.5000 - val_loss: 0.7047\n",
      "Epoch 3/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.8327 - loss: 0.4436 - val_accuracy: 0.7411 - val_loss: 0.7047\n",
      "Epoch 4/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6875 - loss: 0.7733 - val_accuracy: 1.0000 - val_loss: 0.1475\n",
      "Epoch 5/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - accuracy: 0.8981 - loss: 0.2734 - val_accuracy: 0.7768 - val_loss: 0.6849\n",
      "Epoch 6/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8125 - loss: 0.3777 - val_accuracy: 0.8750 - val_loss: 0.5545\n",
      "Epoch 7/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9127 - loss: 0.2132 - val_accuracy: 0.7054 - val_loss: 0.7429\n",
      "Epoch 8/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9375 - loss: 0.1917 - val_accuracy: 0.6250 - val_loss: 0.8304\n",
      "Epoch 9/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 124ms/step - accuracy: 0.9085 - loss: 0.2472 - val_accuracy: 0.7500 - val_loss: 0.7073\n",
      "Epoch 10/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0822 - val_accuracy: 0.7500 - val_loss: 0.3815\n",
      "Epoch 11/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 0.9618 - loss: 0.1210 - val_accuracy: 0.7768 - val_loss: 0.7289\n",
      "Epoch 12/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0257 - val_accuracy: 0.6250 - val_loss: 1.1071\n",
      "\u001b[1m1/7\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.7500 - loss: 0.8905"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 13:54:59.789374: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step - accuracy: 0.7521 - loss: 0.7639\n",
      "Validation loss: 0.7883021235466003\n",
      "Validation accuracy: 0.7410714030265808\n",
      "Total test images being processed: 120\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 247ms/step\n",
      "Predictions saved to preds.txt\n"
     ]
    }
   ],
   "source": [
    "# Model 2 full code\n",
    "# We are using MobileNetV2 as the base model\n",
    "# This is a Transfer Learning approach\n",
    "# It is from the Keras module like Sequential was for CNN\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 16 # from 32 to 16\n",
    "epochs = 12 # from 8 to 12\n",
    "learning_rate = 0.001 # from 0.0001 to 0.001\n",
    "\n",
    "# Preprocessing parameters\n",
    "target_size = (224, 224)  # Resize images to 224x224 pixels\n",
    "\n",
    "# Define directories\n",
    "train_dir = 'yola/split_dataset/train'\n",
    "validation_dir = 'yola/split_dataset/validation'\n",
    "test_dir = 'yola/split_dataset/test'\n",
    "test_filenames_path = 'yola/test_filenames.txt'\n",
    "\n",
    "# Function to preprocess images\n",
    "def preprocess_images(directory, target_size):\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(('jpg', 'jpeg', 'png')):\n",
    "                img_path = os.path.join(root, file)\n",
    "                img = load_img(img_path)\n",
    "                img = img.resize(target_size)\n",
    "                img_array = img_to_array(img)\n",
    "                img = array_to_img(img_array)\n",
    "                img.save(img_path)\n",
    "\n",
    "# Preprocess images in training, validation, and test directories\n",
    "preprocess_images(train_dir, target_size)\n",
    "preprocess_images(validation_dir, target_size)\n",
    "preprocess_images(test_dir, target_size)\n",
    "\n",
    "# Data generators for training, validation, and test datasets\n",
    "datagen = ImageDataGenerator(preprocessing_function=preprocess_input)  # Use MobileNetV2 preprocessing\n",
    "\n",
    "# Load datasets\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=target_size,  # Images are now resized to 224x224 pixels\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=target_size,  # Images are now resized to 224x224 pixels\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Debug information\n",
    "print(f\"Total training images: {train_generator.samples}\")\n",
    "print(f\"Total validation images: {validation_generator.samples}\")\n",
    "\n",
    "# Load the pre-trained MobileNetV2 model from local weights\n",
    "weights_path = './models/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5'\n",
    "base_model = MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights=None)\n",
    "base_model.load_weights(weights_path)\n",
    "base_model.trainable = False  # Freeze the base model\n",
    "\n",
    "# Add custom classification layers on top of the base model\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(3, activation='softmax')(x)  # Assuming 3 classes: bacilli, cocci, spirilla\n",
    "\n",
    "# Create the full model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model with the specified learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size\n",
    ")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "validation_steps = validation_generator.samples // batch_size\n",
    "val_loss, val_accuracy = model.evaluate(validation_generator, steps=validation_steps)\n",
    "print(f\"Validation loss: {val_loss}\")\n",
    "print(f\"Validation accuracy: {val_accuracy}\")\n",
    "\n",
    "# Load test filenames\n",
    "with open(test_filenames_path, 'r') as file:\n",
    "    test_filenames = file.read().splitlines()\n",
    "\n",
    "# Predict on test data\n",
    "test_images = []\n",
    "for filename in test_filenames:\n",
    "    img_path = os.path.join(test_dir, filename)\n",
    "    img = load_img(img_path, target_size=target_size)\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = preprocess_input(img_array)  # Use MobileNetV2 preprocessing\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    test_images.append(img_array)\n",
    "\n",
    "test_images = np.vstack(test_images)\n",
    "\n",
    "print(f\"Total test images being processed: {len(test_images)}\")\n",
    "\n",
    "# Predict on test data\n",
    "predictions = model.predict(test_images)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Save predictions to a file\n",
    "output_file = 'preds.txt'\n",
    "with open(output_file, 'w') as f:\n",
    "    for label in predicted_classes:\n",
    "        f.write(f\"{label}\\n\")\n",
    "\n",
    "print(f\"Predictions saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fe3dbb",
   "metadata": {},
   "source": [
    "## tuning, image size to 150x150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82123211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 360 images belonging to 3 classes.\n",
      "Found 120 images belonging to 3 classes.\n",
      "Total training images: 360\n",
      "Total validation images: 120\n",
      "Epoch 1/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - accuracy: 0.4833 - loss: 1.7878 - val_accuracy: 0.6518 - val_loss: 0.9187\n",
      "Epoch 2/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7500 - loss: 0.6651 - val_accuracy: 0.6250 - val_loss: 0.8804\n",
      "Epoch 3/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8346 - loss: 0.4625 - val_accuracy: 0.7321 - val_loss: 0.8035\n",
      "Epoch 4/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8750 - loss: 0.4381 - val_accuracy: 0.6250 - val_loss: 1.4350\n",
      "Epoch 5/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8125 - loss: 0.4598 - val_accuracy: 0.7054 - val_loss: 0.8293\n",
      "Epoch 6/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7500 - loss: 0.4987 - val_accuracy: 0.8750 - val_loss: 0.4711\n",
      "Epoch 7/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9030 - loss: 0.2436 - val_accuracy: 0.6518 - val_loss: 0.9225\n",
      "Epoch 8/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.1707 - val_accuracy: 0.8750 - val_loss: 0.1842\n",
      "Epoch 9/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - accuracy: 0.9326 - loss: 0.1718 - val_accuracy: 0.6875 - val_loss: 0.8836\n",
      "Epoch 10/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8750 - loss: 0.2035 - val_accuracy: 0.6250 - val_loss: 1.2573\n",
      "Epoch 11/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.9575 - loss: 0.1132 - val_accuracy: 0.6875 - val_loss: 0.9372\n",
      "Epoch 12/12\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0635 - val_accuracy: 0.8750 - val_loss: 0.2673\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.6270 - loss: 1.0955\n",
      "Validation loss: 0.8686134219169617\n",
      "Validation accuracy: 0.6964285969734192\n",
      "Total test images being processed: 120\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 168ms/step\n",
      "Predictions saved to preds.txt\n"
     ]
    }
   ],
   "source": [
    "# Model 2 full code\n",
    "# We are using MobileNetV2 as the base model\n",
    "# This is a Transfer Learning approach\n",
    "# It is from the Keras module like Sequential was for CNN\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 16 # from 32 to 16\n",
    "epochs = 12 # from 8 to 12\n",
    "learning_rate = 0.001 # from 0.0001 to 0.001\n",
    "\n",
    "# Preprocessing parameters\n",
    "target_size = (150, 150)  # Resize images to 150x150 pixels\n",
    "\n",
    "# Define directories\n",
    "train_dir = 'yola/split_dataset/train'\n",
    "validation_dir = 'yola/split_dataset/validation'\n",
    "test_dir = 'yola/split_dataset/test'\n",
    "test_filenames_path = 'yola/test_filenames.txt'\n",
    "\n",
    "# Function to preprocess images\n",
    "def preprocess_images(directory, target_size):\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(('jpg', 'jpeg', 'png')):\n",
    "                img_path = os.path.join(root, file)\n",
    "                img = load_img(img_path)\n",
    "                img = img.resize(target_size)\n",
    "                img_array = img_to_array(img)\n",
    "                img = array_to_img(img_array)\n",
    "                img.save(img_path)\n",
    "\n",
    "# Preprocess images in training, validation, and test directories\n",
    "preprocess_images(train_dir, target_size)\n",
    "preprocess_images(validation_dir, target_size)\n",
    "preprocess_images(test_dir, target_size)\n",
    "\n",
    "# Data generators for training, validation, and test datasets\n",
    "datagen = ImageDataGenerator(preprocessing_function=preprocess_input)  # Use MobileNetV2 preprocessing\n",
    "\n",
    "# Load datasets\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=target_size,  # Images are now resized to 150x150 pixels\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=target_size,  # Images are now resized to 150x150 pixels\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Debug information\n",
    "print(f\"Total training images: {train_generator.samples}\")\n",
    "print(f\"Total validation images: {validation_generator.samples}\")\n",
    "\n",
    "# Load the pre-trained MobileNetV2 model from local weights\n",
    "weights_path = './models/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5'\n",
    "base_model = MobileNetV2(input_shape=(150, 150, 3), include_top=False, weights=None)\n",
    "base_model.load_weights(weights_path)\n",
    "base_model.trainable = False  # Freeze the base model\n",
    "\n",
    "# Add custom classification layers on top of the base model\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(3, activation='softmax')(x)  # Assuming 3 classes: bacilli, cocci, spirilla\n",
    "\n",
    "# Create the full model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model with the specified learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size\n",
    ")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "validation_steps = validation_generator.samples // batch_size\n",
    "val_loss, val_accuracy = model.evaluate(validation_generator, steps=validation_steps)\n",
    "print(f\"Validation loss: {val_loss}\")\n",
    "print(f\"Validation accuracy: {val_accuracy}\")\n",
    "\n",
    "# Load test filenames\n",
    "with open(test_filenames_path, 'r') as file:\n",
    "    test_filenames = file.read().splitlines()\n",
    "\n",
    "# Predict on test data\n",
    "test_images = []\n",
    "for filename in test_filenames:\n",
    "    img_path = os.path.join(test_dir, filename)\n",
    "    img = load_img(img_path, target_size=target_size)\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = preprocess_input(img_array)  # Use MobileNetV2 preprocessing\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    test_images.append(img_array)\n",
    "\n",
    "test_images = np.vstack(test_images)\n",
    "\n",
    "print(f\"Total test images being processed: {len(test_images)}\")\n",
    "\n",
    "# Predict on test data\n",
    "predictions = model.predict(test_images)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Save predictions to a file\n",
    "output_file = 'preds.txt'\n",
    "with open(output_file, 'w') as f:\n",
    "    for label in predicted_classes:\n",
    "        f.write(f\"{label}\\n\")\n",
    "\n",
    "print(f\"Predictions saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959c4a3e",
   "metadata": {},
   "source": [
    "## tuning, image size to 256x256 32/12/.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec16b00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 360 images belonging to 3 classes.\n",
      "Found 120 images belonging to 3 classes.\n",
      "Total training images: 360\n",
      "Total validation images: 120\n",
      "Epoch 1/12\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 289ms/step - accuracy: 0.4977 - loss: 1.2751 - val_accuracy: 0.7604 - val_loss: 0.6688\n",
      "Epoch 2/12\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7188 - loss: 0.7340 - val_accuracy: 0.5833 - val_loss: 0.7965\n",
      "Epoch 3/12\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 269ms/step - accuracy: 0.8049 - loss: 0.4888 - val_accuracy: 0.7396 - val_loss: 0.7364\n",
      "Epoch 4/12\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7500 - loss: 0.7761 - val_accuracy: 0.5833 - val_loss: 1.1743\n",
      "Epoch 5/12\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 276ms/step - accuracy: 0.8779 - loss: 0.3552 - val_accuracy: 0.7083 - val_loss: 0.7917\n",
      "Epoch 6/12\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9688 - loss: 0.2413 - val_accuracy: 0.6667 - val_loss: 1.1393\n",
      "Epoch 7/12\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 272ms/step - accuracy: 0.9239 - loss: 0.2709 - val_accuracy: 0.7917 - val_loss: 0.5946\n",
      "Epoch 8/12\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9375 - loss: 0.2120 - val_accuracy: 0.7917 - val_loss: 0.6765\n",
      "Epoch 9/12\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 262ms/step - accuracy: 0.9351 - loss: 0.1886 - val_accuracy: 0.8125 - val_loss: 0.5325\n",
      "Epoch 10/12\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8750 - loss: 0.2526 - val_accuracy: 0.7500 - val_loss: 0.8984\n",
      "Epoch 11/12\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 245ms/step - accuracy: 0.9545 - loss: 0.1532 - val_accuracy: 0.7604 - val_loss: 0.7025\n",
      "Epoch 12/12\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9375 - loss: 0.1181 - val_accuracy: 0.5417 - val_loss: 1.0957\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 210ms/step - accuracy: 0.6914 - loss: 0.7593\n",
      "Validation loss: 0.840329647064209\n",
      "Validation accuracy: 0.6875\n",
      "Total test images being processed: 120\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 271ms/step\n",
      "Predictions saved to preds.txt\n"
     ]
    }
   ],
   "source": [
    "# Model 2 full code\n",
    "# We are using MobileNetV2 as the base model\n",
    "# This is a Transfer Learning approach\n",
    "# It is from the Keras module like Sequential was for CNN\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32 \n",
    "epochs = 12 \n",
    "learning_rate = 0.001\n",
    "\n",
    "# Preprocessing parameters\n",
    "target_size = (256, 256)  # Resize images to 256x256 pixels\n",
    "\n",
    "# Define directories\n",
    "train_dir = 'yola/split_dataset/train'\n",
    "validation_dir = 'yola/split_dataset/validation'\n",
    "test_dir = 'yola/split_dataset/test'\n",
    "test_filenames_path = 'yola/test_filenames.txt'\n",
    "\n",
    "# Function to preprocess images\n",
    "def preprocess_images(directory, target_size):\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(('jpg', 'jpeg', 'png')):\n",
    "                img_path = os.path.join(root, file)\n",
    "                img = load_img(img_path)\n",
    "                img = img.resize(target_size)\n",
    "                img_array = img_to_array(img)\n",
    "                img = array_to_img(img_array)\n",
    "                img.save(img_path)\n",
    "\n",
    "# Preprocess images in training, validation, and test directories\n",
    "preprocess_images(train_dir, target_size)\n",
    "preprocess_images(validation_dir, target_size)\n",
    "preprocess_images(test_dir, target_size)\n",
    "\n",
    "# Data generators for training, validation, and test datasets\n",
    "datagen = ImageDataGenerator(preprocessing_function=preprocess_input)  # Use MobileNetV2 preprocessing\n",
    "\n",
    "# Load datasets\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=target_size,  # Images are now resized to 256x256 pixels\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=target_size,  # Images are now resized to 256x256 pixels\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Debug information\n",
    "print(f\"Total training images: {train_generator.samples}\")\n",
    "print(f\"Total validation images: {validation_generator.samples}\")\n",
    "\n",
    "# Load the pre-trained MobileNetV2 model from local weights\n",
    "weights_path = './models/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5'\n",
    "base_model = MobileNetV2(input_shape=(256, 256, 3), include_top=False, weights=None)\n",
    "base_model.load_weights(weights_path)\n",
    "base_model.trainable = False  # Freeze the base model\n",
    "\n",
    "# Add custom classification layers on top of the base model\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(3, activation='softmax')(x)  # Assuming 3 classes: bacilli, cocci, spirilla\n",
    "\n",
    "# Create the full model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model with the specified learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size\n",
    ")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "validation_steps = validation_generator.samples // batch_size\n",
    "val_loss, val_accuracy = model.evaluate(validation_generator, steps=validation_steps)\n",
    "print(f\"Validation loss: {val_loss}\")\n",
    "print(f\"Validation accuracy: {val_accuracy}\")\n",
    "\n",
    "# Load test filenames\n",
    "with open(test_filenames_path, 'r') as file:\n",
    "    test_filenames = file.read().splitlines()\n",
    "\n",
    "# Predict on test data\n",
    "test_images = []\n",
    "for filename in test_filenames:\n",
    "    img_path = os.path.join(test_dir, filename)\n",
    "    img = load_img(img_path, target_size=target_size)\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = preprocess_input(img_array)  # Use MobileNetV2 preprocessing\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    test_images.append(img_array)\n",
    "\n",
    "test_images = np.vstack(test_images)\n",
    "\n",
    "print(f\"Total test images being processed: {len(test_images)}\")\n",
    "\n",
    "# Predict on test data\n",
    "predictions = model.predict(test_images)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Save predictions to a file\n",
    "output_file = 'preds.txt'\n",
    "with open(output_file, 'w') as f:\n",
    "    for label in predicted_classes:\n",
    "        f.write(f\"{label}\\n\")\n",
    "\n",
    "print(f\"Predictions saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
