{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sift cnn descriptors pytorch (from replit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Helper class for custom dataset\n",
    "class CustomImageDataset(Dataset):\n",
    "\n",
    "    def __init__(self, image_labels, image_dir, transform=None):\n",
    "        self.image_labels = image_labels\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, f\"{idx+1}.png\")\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        label = self.image_labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Load labels\n",
    "def load_labels(mat_file):\n",
    "    print(\"Loading labels...\")\n",
    "    start_time = time.time()\n",
    "    labels = scipy.io.loadmat(mat_file)['labels'].flatten()\n",
    "    end_time = time.time()\n",
    "    print(f\"Labels loading complete in {end_time - start_time:.2f} seconds.\")\n",
    "    return labels\n",
    "\n",
    "\n",
    "# Preprocess images, extract SIFT features, and visualize\n",
    "def preprocess_images_and_extract_sift(image_labels, image_dir):\n",
    "    print(\"\\nStarting preprocessing and SIFT extraction...\")\n",
    "    sift = cv2.SIFT_create()\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    dataset = CustomImageDataset(image_labels, image_dir, transform)\n",
    "\n",
    "    # Visualize initial images\n",
    "    print(\"Visualizing initial images...\")\n",
    "    for i in range(5):\n",
    "        img, _ = dataset[i]\n",
    "        plt.imshow(img.squeeze(), cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "    # Extract SIFT features\n",
    "    sift_descriptors = []\n",
    "    for idx, (img, _) in enumerate(dataset):\n",
    "        img_np = img.squeeze().numpy().astype(np.uint8)\n",
    "        keypoints, descriptors = sift.detectAndCompute(img_np, None)\n",
    "        sift_descriptors.append((keypoints, descriptors))\n",
    "\n",
    "        # Progress update\n",
    "        if (idx + 1) % 1000 == 0:\n",
    "            print(f\"Processed {idx + 1} images...\")\n",
    "\n",
    "    print(\"SIFT feature extraction complete.\")\n",
    "\n",
    "    # Visualize images with descriptors and keypoints\n",
    "    print(\"Visualizing images with descriptors and keypoints...\")\n",
    "    for i in range(5):\n",
    "        img_np = dataset[i][0].squeeze().numpy().astype(np.uint8)\n",
    "        keypoints, _ = sift_descriptors[i]\n",
    "        img_with_keypoints = cv2.drawKeypoints(img_np, keypoints, None)\n",
    "        plt.imshow(img_with_keypoints, cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "    return sift_descriptors\n",
    "\n",
    "\n",
    "# Setup PyTorch model\n",
    "class SimpleCNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 16 * 16, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)  # assuming 10 classes for this example\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# Load and preprocess images\n",
    "def setup_data_directories():\n",
    "    print(\"Setting up data directories...\")\n",
    "    # Create directories if they don't exist\n",
    "    Path(\"cells\").mkdir(exist_ok=True)\n",
    "    return True\n",
    "\n",
    "\n",
    "# Load labels\n",
    "def download_file(url, local_path):\n",
    "    print(f\"Downloading {local_path}...\")\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(local_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Downloaded {local_path}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Failed to download {local_path}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Download labels\n",
    "def download_dataset(base_url, image_dir, mat_file):\n",
    "    print(\"Starting dataset download...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Setup directories\n",
    "    setup_data_directories()\n",
    "\n",
    "    # Download labels.mat\n",
    "    labels_url = f\"{base_url}/{mat_file}\"\n",
    "    if not download_file(labels_url, mat_file):\n",
    "        return False\n",
    "\n",
    "    # Download images\n",
    "    success_count = 0\n",
    "    for i in range(1, 10001):  # Assuming 10000 images\n",
    "        image_url = f\"{base_url}/{image_dir}/{i}.png\"\n",
    "        local_path = f\"{image_dir}/{i}.png\"\n",
    "\n",
    "        if not os.path.exists(\n",
    "                local_path):  # Only download if file doesn't exist\n",
    "            if download_file(image_url, local_path):\n",
    "                success_count += 1\n",
    "        else:\n",
    "            success_count += 1\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Downloaded {i} images...\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Dataset download completed in {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Successfully downloaded {success_count} images\")\n",
    "    return True\n",
    "\n",
    "# main function\n",
    "def main():\n",
    "    # GitHub raw content base URL\n",
    "    base_url = \"https://raw.githubusercontent.com/RyanS974/RyanS974/main/datasets/hep2cells/\"\n",
    "    image_dir = 'cells'\n",
    "    mat_file = 'labels.mat'\n",
    "\n",
    "    # Download dataset\n",
    "    if not download_dataset(base_url, image_dir, mat_file):\n",
    "        print(\"Failed to download dataset\")\n",
    "        return\n",
    "\n",
    "    # Step 1: Load labels\n",
    "    labels = load_labels(mat_file)\n",
    "\n",
    "    # Step 2: Preprocess and extract SIFT features\n",
    "    descriptors = preprocess_images_and_extract_sift(labels, image_dir)\n",
    "\n",
    "    # Step 3: Split the data\n",
    "    train_labels, temp_labels = train_test_split(labels,\n",
    "                                                 test_size=0.3,\n",
    "                                                 random_state=42)\n",
    "    val_labels, test_labels = train_test_split(temp_labels,\n",
    "                                               test_size=0.5,\n",
    "                                               random_state=42)\n",
    "\n",
    "    print(\n",
    "        f\"Dataset split into train ({len(train_labels)}), \"\n",
    "        f\"validation ({len(val_labels)}), and test ({len(test_labels)}) sets.\")\n",
    "\n",
    "    # Step 4: Define CNN model\n",
    "    model = SimpleCNN()\n",
    "\n",
    "    print(\"Model defined.\")\n",
    "\n",
    "    # Further steps, defining loss, optimizer, and starting training and evaluation.\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
