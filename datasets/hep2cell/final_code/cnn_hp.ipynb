{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN, Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN on the HEp2 Cell Dataset.  Default hyperparameters and GridSearch hyperparameter tuning.\n",
    "\n",
    "The first set of code blocks are of function definitions.  The last ones are calling the functions.  You need to run these first ones in order, the the last set of function calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Related Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading of the matlab file and displaying some information on it, mainly the distribution of the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "\n",
    "def load_labels(mat_file, num_labels=63445):\n",
    "    print(\"Loading labels...\")\n",
    "    labels = scipy.io.loadmat(mat_file)['labels'].flatten()[:num_labels]\n",
    "    print(f\"Loaded {len(labels)} labels.\\n\")\n",
    "    return labels\n",
    "\n",
    "def print_label_distribution(labels):\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    total = len(labels)\n",
    "    print(\"Label Distribution:\")\n",
    "    for label, count in zip(unique, counts):\n",
    "        percentage = (count / total) * 100\n",
    "        print(f\"Label {label}: {count} ({percentage:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing of 64 x 64 image sizes, normalization 0-1 values, and grayscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def preprocess_images(image_dir, num_images):\n",
    "    print(\"Preprocessing images...\")\n",
    "    images = []\n",
    "    for i in range(1, num_images + 1):\n",
    "        img_path = os.path.join(image_dir, f\"{i}.png\")\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Warning: Image {img_path} not found\")\n",
    "            continue\n",
    "\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        image = cv2.resize(image, (64, 64))\n",
    "        image = image / 255.0\n",
    "        images.append(image)\n",
    "\n",
    "        if i % 5000 == 0:\n",
    "            print(f\"Processed images {i-4999}-{i}\")\n",
    "    \n",
    "    print(\"Finished preprocessing images.\\n\")\n",
    "    return np.array(images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splitting, CNN Model Building, Training, Validation, Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we split the dataset, build the cnn model, train it, validate it, and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def train_and_evaluate(X, y, sample_size=25000, learning_rate=0.001, dropout_rate=0.5, conv_filters=32):\n",
    "    \"\"\"\n",
    "    Train and evaluate CNN model with configurable hyperparameters.\n",
    "    \n",
    "    Args:\n",
    "        X: Input features\n",
    "        y: Target labels\n",
    "        sample_size: Number of samples to use\n",
    "        learning_rate: Learning rate for Adam optimizer\n",
    "        dropout_rate: Dropout rate for regularization\n",
    "        conv_filters: Number of filters in first conv layer (second layer will be doubled)\n",
    "    \"\"\"\n",
    "    print(\"Starting training and evaluation...\")\n",
    "\n",
    "    # Convert labels to categorical\n",
    "    y = to_categorical(y)\n",
    "\n",
    "    # Sample 25,000 images directly from the original dataset\n",
    "    X_sampled, _, y_sampled, _ = train_test_split(X, y, train_size=sample_size, random_state=42)\n",
    "    print(f\"Sampled data shape: {X_sampled.shape}\")\n",
    "\n",
    "    # Split the sampled data into new training, validation, and testing datasets\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X_sampled, y_sampled, test_size=0.4, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    print(f\"Sampled data split: {len(X_train)} training, {len(X_val)} validation, {len(X_test)} testing samples\")\n",
    "\n",
    "    # Build the CNN model with configurable hyperparameters\n",
    "    model = Sequential([\n",
    "        Conv2D(conv_filters, (3, 3), activation='relu', input_shape=(64, 64, 1)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(conv_filters * 2, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(y.shape[1], activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Compile the model with configurable learning rate\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), \n",
    "                 loss='categorical_crossentropy', \n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    print(f\"Training CNN model with learning_rate={learning_rate}, dropout_rate={dropout_rate}, conv_filters={conv_filters}\")\n",
    "    model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val), batch_size=32)\n",
    "    print(\"CNN model trained\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(\"Evaluating model...\")\n",
    "    val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
    "    print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "    print(\"Validation Classification Report:\")\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_val_pred = np.argmax(y_val_pred, axis=1)\n",
    "    y_val = np.argmax(y_val, axis=1)\n",
    "    print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Test Accuracy: {test_accuracy}\")\n",
    "    print(\"Test Classification Report:\")\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_test_pred = np.argmax(y_test_pred, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the calling of the functions we just defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading labels...\n",
      "Loaded 63445 labels.\n",
      "\n",
      "Label Distribution:\n",
      "Label 1: 14367 (22.64%)\n",
      "Label 2: 14655 (23.10%)\n",
      "Label 3: 13257 (20.90%)\n",
      "Label 4: 13737 (21.65%)\n",
      "Label 5: 5086 (8.02%)\n",
      "Label 6: 2343 (3.69%)\n",
      "Preprocessing images...\n",
      "Processed images 1-5000\n",
      "Processed images 5001-10000\n",
      "Processed images 10001-15000\n",
      "Processed images 15001-20000\n",
      "Processed images 20001-25000\n",
      "Processed images 25001-30000\n",
      "Processed images 30001-35000\n",
      "Processed images 35001-40000\n",
      "Processed images 40001-45000\n",
      "Processed images 45001-50000\n",
      "Processed images 50001-55000\n",
      "Processed images 55001-60000\n",
      "Finished preprocessing images.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Local directory paths\n",
    "image_dir = 'cells'  # Directory containing images\n",
    "mat_file = 'labels.mat'  # Labels file\n",
    "num_images = 63445\n",
    "\n",
    "# Load labels and preprocess images\n",
    "labels = load_labels(mat_file, num_labels=num_images)\n",
    "print_label_distribution(labels)\n",
    "images = preprocess_images(image_dir, num_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reshape images for CNN\n",
    "X = np.array(images).reshape(-1, 64, 64, 1)\n",
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split, Model Building, Hyperparameters, Training, Validation, Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These settings listed below, that you can comment out and test, do not perform as well as the settings in cnn.ipynb. The best of them is 83% which is close to the best model of 86% in the other cnn python notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 6\n",
      "Starting training and evaluation...\n",
      "Sampled data shape: (25000, 64, 64, 1)\n",
      "Sampled data split: 15000 training, 5000 validation, 5000 testing samples\n",
      "Training CNN model with learning_rate=0.001, dropout_rate=0.5, conv_filters=128\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 156ms/step - accuracy: 0.4397 - loss: 1.3483 - val_accuracy: 0.7028 - val_loss: 0.8027\n",
      "Epoch 2/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 158ms/step - accuracy: 0.6708 - loss: 0.8464 - val_accuracy: 0.7318 - val_loss: 0.6981\n",
      "Epoch 3/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 155ms/step - accuracy: 0.7214 - loss: 0.7170 - val_accuracy: 0.7592 - val_loss: 0.6066\n",
      "Epoch 4/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 156ms/step - accuracy: 0.7427 - loss: 0.6551 - val_accuracy: 0.7838 - val_loss: 0.5769\n",
      "Epoch 5/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 161ms/step - accuracy: 0.7738 - loss: 0.6153 - val_accuracy: 0.8076 - val_loss: 0.5236\n",
      "Epoch 6/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 159ms/step - accuracy: 0.7904 - loss: 0.5465 - val_accuracy: 0.8188 - val_loss: 0.4967\n",
      "Epoch 7/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 155ms/step - accuracy: 0.7913 - loss: 0.5436 - val_accuracy: 0.8116 - val_loss: 0.5010\n",
      "Epoch 8/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 158ms/step - accuracy: 0.8086 - loss: 0.5013 - val_accuracy: 0.8374 - val_loss: 0.4493\n",
      "Epoch 9/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 151ms/step - accuracy: 0.8262 - loss: 0.4599 - val_accuracy: 0.8268 - val_loss: 0.4548\n",
      "Epoch 10/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 154ms/step - accuracy: 0.8230 - loss: 0.4544 - val_accuracy: 0.8344 - val_loss: 0.4523\n",
      "CNN model trained\n",
      "Evaluating model...\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 41ms/step - accuracy: 0.8415 - loss: 0.4296\n",
      "Validation Accuracy: 0.8343999981880188\n",
      "Validation Classification Report:\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.88      0.83      1112\n",
      "           2       0.78      0.72      0.75      1193\n",
      "           3       0.89      0.87      0.88      1001\n",
      "           4       0.90      0.93      0.92      1104\n",
      "           5       0.78      0.78      0.78       405\n",
      "           6       0.94      0.68      0.79       185\n",
      "\n",
      "    accuracy                           0.83      5000\n",
      "   macro avg       0.85      0.81      0.82      5000\n",
      "weighted avg       0.84      0.83      0.83      5000\n",
      "\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.8259 - loss: 0.4952\n",
      "Test Accuracy: 0.8285999894142151\n",
      "Test Classification Report:\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 41ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.76      0.88      0.82      1125\n",
      "           2       0.79      0.69      0.74      1202\n",
      "           3       0.91      0.86      0.89      1037\n",
      "           4       0.88      0.94      0.91      1067\n",
      "           5       0.76      0.74      0.75       397\n",
      "           6       0.92      0.76      0.83       172\n",
      "\n",
      "    accuracy                           0.83      5000\n",
      "   macro avg       0.84      0.81      0.82      5000\n",
      "weighted avg       0.83      0.83      0.83      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "\n",
    "# Ensure y has the correct number of classes\n",
    "num_classes = len(np.unique(y))\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "#train_and_evaluate(X, y)\n",
    "\n",
    "# Conservative/lightweight setup\n",
    "#train_and_evaluate(X, y, learning_rate=0.0001, dropout_rate=0.3, conv_filters=16)\n",
    "\n",
    "# Aggressive learning setup\n",
    "#train_and_evaluate(X, y, learning_rate=0.01, dropout_rate=0.4, conv_filters=64)\n",
    "\n",
    "# Complex feature detection setup\n",
    "train_and_evaluate(X, y, learning_rate=0.001, dropout_rate=0.5, conv_filters=128)\n",
    "# this setting performs the best of these pre-defined settings.  83% accuracy on the test set.\n",
    "\n",
    "# Careful learning setup\n",
    "#train_and_evaluate(X, y, learning_rate=0.00001, dropout_rate=0.6, conv_filters=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "83% is not bad, and the best of these. The other file has am 86% accuracy model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
