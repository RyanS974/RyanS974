{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN, Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN on the HEp2 Cell Dataset.  Default hyperparameters and GridSearch hyperparameter tuning.\n",
    "\n",
    "The first set of code blocks are of function definitions.  The last ones are calling the functions.  You need to run these first ones in order, the the last set of function calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Related Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading of the matlab file and displaying some information on it, mainly the distribution of the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "\n",
    "def load_labels(mat_file, num_labels=63445):\n",
    "    print(\"Loading labels...\")\n",
    "    labels = scipy.io.loadmat(mat_file)['labels'].flatten()[:num_labels]\n",
    "    print(f\"Loaded {len(labels)} labels.\\n\")\n",
    "    return labels\n",
    "\n",
    "def print_label_distribution(labels):\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    total = len(labels)\n",
    "    print(\"Label Distribution:\")\n",
    "    for label, count in zip(unique, counts):\n",
    "        percentage = (count / total) * 100\n",
    "        print(f\"Label {label}: {count} ({percentage:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing of 64 x 64 image sizes, normalization 0-1 values, and grayscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def preprocess_images(image_dir, num_images):\n",
    "    print(\"Preprocessing images...\")\n",
    "    images = []\n",
    "    for i in range(1, num_images + 1):\n",
    "        img_path = os.path.join(image_dir, f\"{i}.png\")\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Warning: Image {img_path} not found\")\n",
    "            continue\n",
    "\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        image = cv2.resize(image, (64, 64))\n",
    "        image = image / 255.0\n",
    "        images.append(image)\n",
    "\n",
    "        if i % 5000 == 0:\n",
    "            print(f\"Processed images {i-4999}-{i}\")\n",
    "    \n",
    "    print(\"Finished preprocessing images.\\n\")\n",
    "    return np.array(images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splitting, CNN Model Building, Training, Validation, Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we split the dataset, build the cnn model, train it, validate it, and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def train_and_evaluate(X, y, sample_size=63000):\n",
    "    print(\"Starting training and evaluation...\")\n",
    "\n",
    "    # Convert labels to categorical\n",
    "    y = to_categorical(y)\n",
    "\n",
    "    # Sample images directly from the original dataset\n",
    "    X_sampled, _, y_sampled, _ = train_test_split(X, y, train_size=sample_size, random_state=42)\n",
    "    print(f\"Sampled data shape: {X_sampled.shape}\")\n",
    "\n",
    "    # Split the sampled data into new training, validation, and testing datasets\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X_sampled, y_sampled, test_size=0.4, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    print(f\"Sampled data split: {len(X_train)} training, {len(X_val)} validation, {len(X_test)} testing samples\")\n",
    "\n",
    "    # Build the CNN model\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(y.shape[1], activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Training CNN model...\")\n",
    "    model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val), batch_size=32)\n",
    "    print(\"CNN model trained\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(\"Evaluating model...\")\n",
    "    val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
    "    print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "    print(\"Validation Classification Report:\")\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_val_pred = np.argmax(y_val_pred, axis=1)\n",
    "    y_val = np.argmax(y_val, axis=1)\n",
    "    print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Test Accuracy: {test_accuracy}\")\n",
    "    print(\"Test Classification Report:\")\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_test_pred = np.argmax(y_test_pred, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "    print(classification_report(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the calling of the functions we just defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading labels...\n",
      "Loaded 63445 labels.\n",
      "\n",
      "Label Distribution:\n",
      "Label 1: 14367 (22.64%)\n",
      "Label 2: 14655 (23.10%)\n",
      "Label 3: 13257 (20.90%)\n",
      "Label 4: 13737 (21.65%)\n",
      "Label 5: 5086 (8.02%)\n",
      "Label 6: 2343 (3.69%)\n",
      "Preprocessing images...\n",
      "Processed images 1-5000\n",
      "Processed images 5001-10000\n",
      "Processed images 10001-15000\n",
      "Processed images 15001-20000\n",
      "Processed images 20001-25000\n",
      "Processed images 25001-30000\n",
      "Processed images 30001-35000\n",
      "Processed images 35001-40000\n",
      "Processed images 40001-45000\n",
      "Processed images 45001-50000\n",
      "Processed images 50001-55000\n",
      "Processed images 55001-60000\n",
      "Finished preprocessing images.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Local directory paths\n",
    "image_dir = 'cells'  # Directory containing images\n",
    "mat_file = 'labels.mat'  # Labels file\n",
    "num_images = 63445\n",
    "\n",
    "# Load labels and preprocess images\n",
    "labels = load_labels(mat_file, num_labels=num_images)\n",
    "print_label_distribution(labels)\n",
    "images = preprocess_images(image_dir, num_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reshape images for CNN\n",
    "X = np.array(images).reshape(-1, 64, 64, 1)\n",
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split, Model Building, Training, Validation, and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training and evaluation...\n",
      "Sampled data shape: (63000, 64, 64, 1)\n",
      "Sampled data split: 37800 training, 12600 validation, 12600 testing samples\n",
      "Training CNN model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1182/1182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 25ms/step - accuracy: 0.5317 - loss: 1.1501 - val_accuracy: 0.7543 - val_loss: 0.6695\n",
      "Epoch 2/10\n",
      "\u001b[1m1182/1182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 24ms/step - accuracy: 0.7372 - loss: 0.6919 - val_accuracy: 0.7916 - val_loss: 0.5809\n",
      "Epoch 3/10\n",
      "\u001b[1m1182/1182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 24ms/step - accuracy: 0.7760 - loss: 0.6010 - val_accuracy: 0.7936 - val_loss: 0.5326\n",
      "Epoch 4/10\n",
      "\u001b[1m1182/1182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 25ms/step - accuracy: 0.7971 - loss: 0.5434 - val_accuracy: 0.8447 - val_loss: 0.4547\n",
      "Epoch 5/10\n",
      "\u001b[1m1182/1182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 24ms/step - accuracy: 0.8133 - loss: 0.5032 - val_accuracy: 0.8477 - val_loss: 0.4426\n",
      "Epoch 6/10\n",
      "\u001b[1m1182/1182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 25ms/step - accuracy: 0.8243 - loss: 0.4797 - val_accuracy: 0.8544 - val_loss: 0.4319\n",
      "Epoch 7/10\n",
      "\u001b[1m1182/1182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 24ms/step - accuracy: 0.8303 - loss: 0.4537 - val_accuracy: 0.8525 - val_loss: 0.4218\n",
      "Epoch 8/10\n",
      "\u001b[1m1182/1182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 26ms/step - accuracy: 0.8460 - loss: 0.4172 - val_accuracy: 0.8623 - val_loss: 0.4082\n",
      "Epoch 9/10\n",
      "\u001b[1m1182/1182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 24ms/step - accuracy: 0.8528 - loss: 0.3997 - val_accuracy: 0.8593 - val_loss: 0.4115\n",
      "Epoch 10/10\n",
      "\u001b[1m1182/1182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 24ms/step - accuracy: 0.8545 - loss: 0.3875 - val_accuracy: 0.8638 - val_loss: 0.4073\n",
      "CNN model trained\n",
      "Evaluating model...\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8663 - loss: 0.4124\n",
      "Validation Accuracy: 0.8638095259666443\n",
      "Validation Classification Report:\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.91      0.85      2810\n",
      "           2       0.84      0.76      0.79      3026\n",
      "           3       0.92      0.88      0.90      2587\n",
      "           4       0.92      0.94      0.93      2754\n",
      "           5       0.85      0.80      0.82       989\n",
      "           6       0.81      0.86      0.83       434\n",
      "\n",
      "    accuracy                           0.86     12600\n",
      "   macro avg       0.86      0.86      0.86     12600\n",
      "weighted avg       0.87      0.86      0.86     12600\n",
      "\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8555 - loss: 0.4143\n",
      "Test Accuracy: 0.8564285635948181\n",
      "Test Classification Report:\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.90      0.85      2882\n",
      "           2       0.81      0.73      0.77      2879\n",
      "           3       0.91      0.89      0.90      2617\n",
      "           4       0.92      0.94      0.93      2759\n",
      "           5       0.85      0.77      0.81      1024\n",
      "           6       0.80      0.87      0.83       439\n",
      "\n",
      "    accuracy                           0.86     12600\n",
      "   macro avg       0.85      0.85      0.85     12600\n",
      "weighted avg       0.86      0.86      0.86     12600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "train_and_evaluate(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy for validation and testing of about 86%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
