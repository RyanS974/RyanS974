{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sift cnn descriptors 3, load and save image preprocessing and descriptors code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading File Names and Labels\n",
      "Number of file names loaded: 63445\n",
      "Number of labels loaded: 63445\n",
      "Number of selected file names: 2000\n",
      "Number of selected labels: 2000\n",
      "Loading Data - Start Time: 1732419429.90, End Time: 1732419430.44, Duration: 0.54 seconds\n",
      "\n",
      "Splitting Data into Training, Validation, and Testing Sets\n",
      "Number of training files: 800\n",
      "Number of validation files: 600\n",
      "Number of testing files: 600\n",
      "Splitting Data - Start Time: 1732419430.44, End Time: 1732419430.44, Duration: 0.00 seconds\n",
      "\n",
      "Preprocessing Images\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 160\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPreprocessing Images\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    158\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 160\u001b[0m train_images \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://raw.githubusercontent.com/RyanS974/RyanS974/main/datasets/hep2cell/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfile\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_files\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m val_images \u001b[38;5;241m=\u001b[39m preprocess_images([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/RyanS974/RyanS974/main/datasets/hep2cell/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m val_files], target_size)\n\u001b[1;32m    162\u001b[0m test_images \u001b[38;5;241m=\u001b[39m preprocess_images([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/RyanS974/RyanS974/main/datasets/hep2cell/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m test_files], target_size)\n",
      "Cell \u001b[0;32mIn[4], line 114\u001b[0m, in \u001b[0;36mpreprocess_images\u001b[0;34m(image_paths, target_size)\u001b[0m\n\u001b[1;32m    112\u001b[0m total_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(image_paths)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(image_paths):\n\u001b[0;32m--> 114\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_image_for_sift\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         images\u001b[38;5;241m.\u001b[39mappend(image)\n",
      "Cell \u001b[0;32mIn[4], line 90\u001b[0m, in \u001b[0;36mpreprocess_image_for_sift\u001b[0;34m(image_url, target_size, retries)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(retries):\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;66;03m# Download the image from the URL\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m         response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m     92\u001b[0m         image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(BytesIO(response\u001b[38;5;241m.\u001b[39mcontent))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/urllib3/connection.py:507\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1395\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1393\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1394\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1395\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1396\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1397\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:325\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 325\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:286\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 286\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1314\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1312\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1313\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1166\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D, Input, concatenate, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "# Function to print timing information\n",
    "def print_timing_info(phase, start_time, end_time):\n",
    "    duration = end_time - start_time\n",
    "    print(f\"{phase} - Start Time: {start_time:.2f}, End Time: {end_time:.2f}, Duration: {duration:.2f} seconds\")\n",
    "\n",
    "# Load File Names and Labels\n",
    "print(\"Loading File Names and Labels\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Load file names from GitHub\n",
    "github_url = \"https://raw.githubusercontent.com/RyanS974/RyanS974/main/datasets/hep2cell/cells2.txt\"\n",
    "response = requests.get(github_url)\n",
    "file_names = response.text.splitlines()\n",
    "\n",
    "# Load labels from labels.mat using SciPy\n",
    "labels_url = \"https://raw.githubusercontent.com/RyanS974/RyanS974/main/datasets/hep2cell/labels.mat\"\n",
    "labels_path = tf.keras.utils.get_file(\"labels.mat\", labels_url)\n",
    "labels = scipy.io.loadmat(labels_path)['labels'].flatten()\n",
    "\n",
    "# Display the number of file names and labels loaded\n",
    "print(f\"Number of file names loaded: {len(file_names)}\")\n",
    "print(f\"Number of labels loaded: {len(labels)}\")\n",
    "\n",
    "# Ensure that the number of file names matches the number of labels\n",
    "assert len(file_names) == len(labels), \"Mismatch between number of file names and labels\"\n",
    "\n",
    "# Filter the file names to ensure they are within the range 1-2,000\n",
    "filtered_file_names = [file for file in file_names if int(os.path.basename(file).split('.')[0]) <= 2000]\n",
    "\n",
    "# Select the first 2,000 images from the filtered dataset\n",
    "selected_file_names = filtered_file_names[:2000]\n",
    "selected_labels = labels[:2000]\n",
    "\n",
    "# Display the number of selected file names and labels\n",
    "print(f\"Number of selected file names: {len(selected_file_names)}\")\n",
    "print(f\"Number of selected labels: {len(selected_labels)}\")\n",
    "\n",
    "# Ensure that the number of selected file names matches the number of selected labels\n",
    "assert len(selected_file_names) == len(selected_labels), \"Mismatch between number of selected file names and labels\"\n",
    "\n",
    "end_time = time.time()\n",
    "print_timing_info(\"Loading Data\", start_time, end_time)\n",
    "\n",
    "# Split Data into Training, Validation, and Testing Sets\n",
    "print(\"\\nSplitting Data into Training, Validation, and Testing Sets\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Split the selected file names and labels into training, validation, and testing sets\n",
    "train_files, temp_files, train_labels, temp_labels = train_test_split(\n",
    "    selected_file_names, selected_labels, test_size=0.6, random_state=42)\n",
    "\n",
    "val_files, test_files, val_labels, test_labels = train_test_split(\n",
    "    temp_files, temp_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# Display the number of files in each set\n",
    "print(f\"Number of training files: {len(train_files)}\")\n",
    "print(f\"Number of validation files: {len(val_files)}\")\n",
    "print(f\"Number of testing files: {len(test_files)}\")\n",
    "\n",
    "# Ensure that the total number of files matches the number of selected files\n",
    "assert len(train_files) + len(val_files) + len(test_files) == len(selected_file_names), \"Mismatch in total number of files after splitting\"\n",
    "\n",
    "end_time = time.time()\n",
    "print_timing_info(\"Splitting Data\", start_time, end_time)\n",
    "\n",
    "# Define the target image size\n",
    "target_size = (96, 96)\n",
    "\n",
    "# Function to download and preprocess images: resize, convert to grayscale, and normalize\n",
    "def preprocess_image_for_sift(image_url, target_size, retries=3):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            # Download the image from the URL\n",
    "            response = requests.get(image_url)\n",
    "            response.raise_for_status()\n",
    "            image = Image.open(BytesIO(response.content))\n",
    "            # Convert the image to RGB\n",
    "            image = image.convert('RGB')\n",
    "            # Resize the image to the target size\n",
    "            image_resized = image.resize(target_size)\n",
    "            # Convert the image to grayscale\n",
    "            image_gray = image_resized.convert('L')\n",
    "            # Normalize the pixel values to the range [0, 1]\n",
    "            image_normalized = np.array(image_gray) / 255.0\n",
    "            # Convert to uint8\n",
    "            image_uint8 = (image_normalized * 255).astype(np.uint8)\n",
    "            return image_uint8\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            if attempt == retries - 1:\n",
    "                raise\n",
    "\n",
    "# Preprocess images with progress reporting\n",
    "def preprocess_images(image_paths, target_size):\n",
    "    images = []\n",
    "    total_images = len(image_paths)\n",
    "    for i, path in enumerate(image_paths):\n",
    "        image = preprocess_image_for_sift(path, target_size)\n",
    "        if image is not None:\n",
    "            images.append(image)\n",
    "        if (i + 1) % 500 == 0:\n",
    "            percent_done = ((i + 1) / total_images) * 100\n",
    "            print(f\"Processed {i + 1} of {total_images} images ({percent_done:.2f}%)\")\n",
    "    return np.array(images)\n",
    "\n",
    "# Extract SIFT descriptors with progress reporting\n",
    "def extract_sift_descriptors(images):\n",
    "    sift = cv2.SIFT_create()\n",
    "    all_descriptors = []\n",
    "    total_images = len(images)\n",
    "    for i, img in enumerate(images):\n",
    "        img_uint8 = (img * 255).astype(np.uint8)\n",
    "        keypoints, descriptors = sift.detectAndCompute(img_uint8, None)\n",
    "        if descriptors is None:\n",
    "            descriptors = np.zeros((1, 128))\n",
    "        all_descriptors.append(descriptors)\n",
    "        if (i + 1) % 500 == 0:\n",
    "            percent_done = ((i + 1) / total_images) * 100\n",
    "            print(f\"Extracted SIFT descriptors for {i + 1} of {total_images} images ({percent_done:.2f}%)\")\n",
    "    return all_descriptors\n",
    "\n",
    "# Check if preprocessed images and descriptors exist\n",
    "preprocessed_data_exists = os.path.exists('train_images.npy') and os.path.exists('val_images.npy') and os.path.exists('test_images.npy') and os.path.exists('train_descriptors.npy') and os.path.exists('val_descriptors.npy') and os.path.exists('test_descriptors.npy')\n",
    "\n",
    "if preprocessed_data_exists:\n",
    "    # Load preprocessed images and descriptors\n",
    "    print(\"\\nLoading Preprocessed Data\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_images = np.load('train_images.npy')\n",
    "    val_images = np.load('val_images.npy')\n",
    "    test_images = np.load('test_images.npy')\n",
    "    train_descriptors = np.load('train_descriptors.npy', allow_pickle=True)\n",
    "    val_descriptors = np.load('val_descriptors.npy', allow_pickle=True)\n",
    "    test_descriptors = np.load('test_descriptors.npy', allow_pickle=True)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print_timing_info(\"Loading Preprocessed Data\", start_time, end_time)\n",
    "else:\n",
    "    # Preprocess training images\n",
    "    print(\"\\nPreprocessing Images\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_images = preprocess_images([f\"https://raw.githubusercontent.com/RyanS974/RyanS974/main/datasets/hep2cell/{file}\" for file in train_files], target_size)\n",
    "    val_images = preprocess_images([f\"https://raw.githubusercontent.com/RyanS974/RyanS974/main/datasets/hep2cell/{file}\" for file in val_files], target_size)\n",
    "    test_images = preprocess_images([f\"https://raw.githubusercontent.com/RyanS974/RyanS974/main/datasets/hep2cell/{file}\" for file in test_files], target_size)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print_timing_info(\"Preprocessing Images\", start_time, end_time)\n",
    "    \n",
    "    # Save preprocessed images\n",
    "    np.save('train_images.npy', train_images)\n",
    "    np.save('val_images.npy', val_images)\n",
    "    np.save('test_images.npy', test_images)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "train_images = np.array(train_images)\n",
    "val_images = np.array(val_images)\n",
    "test_images = np.array(test_images)\n",
    "\n",
    "# Display the shapes of the preprocessed image arrays\n",
    "print(f\"Shape of training images: {train_images.shape}\")\n",
    "print(f\"Shape of validation images: {val_images.shape}\")\n",
    "print(f\"Shape of testing images: {test_images.shape}\")\n",
    "\n",
    "# Ensure that the number of images matches the number of labels in each set\n",
    "assert train_images.shape[0] == len(train_labels), \"Mismatch between number of training images and labels\"\n",
    "assert val_images.shape[0] == len(val_labels), \"Mismatch between number of validation images and labels\"\n",
    "assert test_images.shape[0] == len(test_labels), \"Mismatch between number of testing images and labels\"\n",
    "\n",
    "# Visualize 5 images after loading\n",
    "for i in range(5):\n",
    "    plt.imshow(train_images[i], cmap='gray')\n",
    "    plt.title(f\"Training Image {i+1}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Extract SIFT Descriptors\n",
    "if not preprocessed_data_exists:\n",
    "    print(\"\\nExtracting SIFT Descriptors\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_descriptors = extract_sift_descriptors(train_images)\n",
    "    val_descriptors = extract_sift_descriptors(val_images)\n",
    "    test_descriptors = extract_sift_descriptors(test_images)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print_timing_info(\"Extracting SIFT Descriptors\", start_time, end_time)\n",
    "    \n",
    "    # Save extracted descriptors\n",
    "    np.save('train_descriptors.npy', train_descriptors)\n",
    "    np.save('val_descriptors.npy', val_descriptors)\n",
    "    np.save('test_descriptors.npy', test_descriptors)\n",
    "\n",
    "# Filter images and labels to match the descriptors\n",
    "train_images = train_images[:len(train_descriptors)]\n",
    "train_labels = train_labels[:len(train_descriptors)]\n",
    "val_images = val_images[:len(val_descriptors)]\n",
    "val_labels = val_labels[:len(val_descriptors)]\n",
    "test_images = test_images[:len(test_descriptors)]\n",
    "test_labels = test_labels[:len(test_descriptors)]\n",
    "\n",
    "# Calculate percentages of images with descriptors\n",
    "train_percentage = (len(train_descriptors) / len(train_images)) * 100\n",
    "val_percentage = (len(val_descriptors) / len(val_images)) * 100\n",
    "test_percentage = (len(test_descriptors) / len(test_images)) * 100\n",
    "\n",
    "# Calculate average number of descriptors per image\n",
    "train_avg_descriptors = np.mean([len(desc) for desc in train_descriptors])\n",
    "val_avg_descriptors = np.mean([len(desc) for desc in val_descriptors])\n",
    "test_avg_descriptors = np.mean([len(desc) for desc in test_descriptors])\n",
    "\n",
    "# Display the statistics\n",
    "print(f\"Number of descriptors for the first training image: {len(train_descriptors[0]) if train_descriptors[0] is not None else 0}\")\n",
    "print(f\"Number of training images with descriptors: {len(train_descriptors)} ({train_percentage:.2f}%)\")\n",
    "print(f\"Number of validation images with descriptors: {len(val_descriptors)} ({val_percentage:.2f}%)\")\n",
    "print(f\"Number of testing images with descriptors: {len(test_descriptors)} ({test_percentage:.2f}%)\")\n",
    "print(f\"Average number of descriptors per training image: {train_avg_descriptors:.2f}\")\n",
    "print(f\"Average number of descriptors per validation image: {val_avg_descriptors:.2f}\")\n",
    "print(f\"Average number of descriptors per testing image: {test_avg_descriptors:.2f}\")\n",
    "\n",
    "# Ensure that descriptors are extracted for all images\n",
    "assert len(train_descriptors) > 0, \"Failed to extract descriptors for all training images\"\n",
    "assert len(val_descriptors) > 0, \"Failed to extract descriptors for all validation images\"\n",
    "assert len(test_descriptors) > 0, \"Failed to extract descriptors for all testing images\"\n",
    "\n",
    "# Function to pad descriptors to a fixed size\n",
    "def pad_descriptors(descriptors, max_descriptors):\n",
    "    padded_descriptors = np.zeros((max_descriptors, 128))\n",
    "    if descriptors is not None and len(descriptors) > 0:\n",
    "        num_desc = min(len(descriptors), max_descriptors)\n",
    "        padded_descriptors[:num_desc, :] = descriptors[:num_desc, :]\n",
    "    return padded_descriptors\n",
    "\n",
    "# Define the maximum number of descriptors\n",
    "max_descriptors = 4000  # Adjust based on your dataset\n",
    "\n",
    "# Pad descriptors for training, validation, and testing sets\n",
    "train_padded = np.array([pad_descriptors(desc, max_descriptors) if desc is not None else np.zeros((max_descriptors, 128)) for desc in train_descriptors])\n",
    "val_padded = np.array([pad_descriptors(desc, max_descriptors) if desc is not None else np.zeros((max_descriptors, 128)) for desc in val_descriptors])\n",
    "test_padded = np.array([pad_descriptors(desc, max_descriptors) if desc is not None else np.zeros((max_descriptors, 128)) for desc in test_descriptors])\n",
    "\n",
    "# Ensure that the number of padded descriptors matches the number of images\n",
    "assert train_padded.shape[0] == train_images.shape[0], \"Mismatch between number of training images and padded descriptors\"\n",
    "assert val_padded.shape[0] == val_images.shape[0], \"Mismatch between number of validation images and padded descriptors\"\n",
    "assert test_padded.shape[0] == test_images.shape[0], \"Mismatch between number of testing images and padded descriptors\"\n",
    "\n",
    "# Reshape images to match the input shape of the model\n",
    "train_images = train_images.reshape((train_images.shape[0], 96, 96, 1))\n",
    "val_images = val_images.reshape((val_images.shape[0], 96, 96, 1))\n",
    "test_images = test_images.reshape((test_images.shape[0], 96, 96, 1))\n",
    "\n",
    "# Reshape padded descriptors to match the input shape of the model\n",
    "train_padded = train_padded.reshape((train_padded.shape[0], max_descriptors, 128, 1))\n",
    "val_padded = val_padded.reshape((val_padded.shape[0], max_descriptors, 128, 1))\n",
    "test_padded = test_padded.reshape((test_padded.shape[0], max_descriptors, 128, 1))\n",
    "\n",
    "# Define the CNN model for image data\n",
    "image_input = Input(shape=(96, 96, 1), name='image_input')\n",
    "x = Conv2D(32, (3, 3), activation='relu')(image_input)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "image_output = Dense(128, activation='relu')(x)\n",
    "\n",
    "# Define the model for SIFT descriptors\n",
    "desc_input = Input(shape=(max_descriptors, 128, 1), name='desc_input')\n",
    "y = Conv2D(32, (3, 3), activation='relu')(desc_input)\n",
    "y = MaxPooling2D((2, 2))(y)\n",
    "y = Conv2D(64, (3, 3), activation='relu')(y)\n",
    "y = MaxPooling2D((2, 2))(y)\n",
    "y = Flatten()(y)\n",
    "y = Dense(128, activation='relu')(y)\n",
    "desc_output = Dense(128, activation='relu')(y)\n",
    "\n",
    "# Concatenate the outputs of both branches\n",
    "combined = concatenate([image_output, desc_output])\n",
    "\n",
    "# Add final classification layers\n",
    "z = Dense(128, activation='relu')(combined)\n",
    "z = Dropout(0.5)(z)  # Add dropout for regularization\n",
    "z = Dense(1, activation='sigmoid')(z)\n",
    "\n",
    "# Define the final model\n",
    "model = Model(inputs=[image_input, desc_input], outputs=z)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the Model\n",
    "print(\"\\nTraining the Model\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the CNN model using the training data\n",
    "history = model.fit(\n",
    "    [train_images, train_padded], train_labels,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=([val_images, val_padded], val_labels)\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print_timing_info(\"Training Model\", start_time, end_time)\n",
    "\n",
    "# Display the training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Visualize SIFT Descriptors\n",
    "\n",
    "# Function to visualize SIFT descriptors on images\n",
    "def visualize_sift(image, keypoints, max_descriptors=5):\n",
    "    overlay = cv2.drawKeypoints(image, keypoints[:max_descriptors], None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    plt.imshow(overlay, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize SIFT descriptors for 3 images\n",
    "for i in range(3):\n",
    "    keypoints, _ = extract_sift_descriptors(train_images[i])\n",
    "    visualize_sift(train_images[i], keypoints)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
